
@misc{everitt_agent_2021,
	title = {Agent Incentives: A Causal Perspective},
	url = {http://arxiv.org/abs/2102.01685},
	shorttitle = {Agent Incentives},
	abstract = {We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an {AI} system.},
	number = {{arXiv}:2102.01685},
	publisher = {{arXiv}},
	author = {Everitt, Tom and Carey, Ryan and Langlois, Eric and Ortega, Pedro A. and Legg, Shane},
	urldate = {2024-01-29},
	date = {2021-03-15},
	eprinttype = {arxiv},
	eprint = {2102.01685 [cs]},
	keywords = {sequential decision making, causality, reinforcement learning, causal discovery, agents},
	file = {arXiv Fulltext PDF:/Users/ep/Zotero/storage/8SF6BRFJ/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf:application/pdf;arXiv.org Snapshot:/Users/ep/Zotero/storage/F2LEFIMM/2102.html:text/html},
}

@misc{carroll_characterizing_2023,
	title = {Characterizing Manipulation from {AI} Systems},
	url = {http://arxiv.org/abs/2303.09387},
	abstract = {Manipulation is a common concern in many domains, such as social media, advertising, and chatbots. As {AI} systems mediate more of our interactions with the world, it is important to understand the degree to which {AI} systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring manipulation in the context of {AI} systems. Firstly, we build upon prior literature on manipulation from other fields and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, harm, and covertness. We review proposals on how to operationalize each factor. Second, we propose a definition of manipulation based on our characterization: a system is manipulative if it acts as if it were pursuing an incentive to change a human (or another agent) intentionally and covertly. Third, we discuss the connections between manipulation and related concepts, such as deception and coercion. Finally, we contextualize our operationalization of manipulation in some applications. Our overall assessment is that while some progress has been made in defining and measuring manipulation from {AI} systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that {AI} systems learn to manipulate humans without the intent of the system designers. We argue that such manipulation poses a significant threat to human autonomy, suggesting that precautionary actions to mitigate it are warranted.},
	number = {{arXiv}:2303.09387},
	publisher = {{arXiv}},
	author = {Carroll, Micah and Chan, Alan and Ashton, Henry and Krueger, David},
	urldate = {2024-01-20},
	date = {2023-10-30},
	eprinttype = {arxiv},
	eprint = {2303.09387 [cs]},
	keywords = {language models, behavioural psychology, dangerous capabilities, manipulation},
	file = {arXiv Fulltext PDF:/Users/ep/Zotero/storage/MRURTLM7/Carroll et al. - 2023 - Characterizing Manipulation from AI Systems.pdf:application/pdf;arXiv.org Snapshot:/Users/ep/Zotero/storage/QRLJQA8A/2303.html:text/html},
}

@misc{richens_robust_2024,
	title = {Robust agents learn causal world models},
	url = {http://arxiv.org/abs/2402.10877},
	doi = {10.48550/arXiv.2402.10877},
	abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
	number = {{arXiv}:2402.10877},
	publisher = {{arXiv}},
	author = {Richens, Jonathan and Everitt, Tom},
	urldate = {2024-11-12},
	date = {2024-07-19},
	eprinttype = {arxiv},
	eprint = {2402.10877},
	keywords = {sequential decision making, causality, reinforcement learning, agents, world models},
	file = {Preprint PDF:/Users/ep/Zotero/storage/WI4JEJB6/Richens and Everitt - 2024 - Robust agents learn causal world models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/54RJB8PK/2402.html:text/html},
}

@misc{casper_defending_2024,
	title = {Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
	url = {http://arxiv.org/abs/2403.05030},
	doi = {10.48550/arXiv.2403.05030},
	abstract = {Despite extensive diagnostics and debugging by developers, {AI} systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training ({AT}) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training ({LAT}) to defend against vulnerabilities without leveraging knowledge of what they are or using inputs that elicit them. {LAT} makes use of the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. Here, we use it to defend against failure modes without examples that elicit them. Specifically, we use {LAT} to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that {LAT} usually improves both robustness to novel attacks and performance on clean data relative to {AT}. This suggests that {LAT} can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
	number = {{arXiv}:2403.05030},
	publisher = {{arXiv}},
	author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
	urldate = {2025-01-31},
	date = {2024-08-22},
	eprinttype = {arxiv},
	eprint = {2403.05030 [cs]},
	keywords = {language models, robustness, interpretability, adversarial},
	file = {Preprint PDF:/Users/ep/Zotero/storage/C6IBB255/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with Latent Adversarial Training.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/HSCHNPAQ/2403.html:text/html},
}

@misc{yi_latent-space_2025,
	title = {Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks},
	url = {http://arxiv.org/abs/2501.10639},
	doi = {10.48550/arXiv.2501.10639},
	abstract = {Ensuring safety alignment has become a critical requirement for large language models ({LLMs}), particularly given their widespread deployment in real-world applications. However, {LLMs} remain susceptible to jailbreak attacks, which exploit system vulnerabilities to bypass safety measures and generate harmful outputs. Although numerous defense mechanisms based on adversarial training have been proposed, a persistent challenge lies in the exacerbation of over-refusal behaviors, which compromise the overall utility of the model. To address these challenges, we propose a Latent-space Adversarial Training with Post-aware Calibration ({LATPC}) framework. During the adversarial training phase, {LATPC} compares harmful and harmless instructions in the latent space and extracts safety-critical dimensions to construct refusal features attack, precisely simulating agnostic jailbreak attack types requiring adversarial mitigation. At the inference stage, an embedding-level calibration mechanism is employed to alleviate over-refusal behaviors with minimal computational overhead. Experimental results demonstrate that, compared to various defense methods across five types of jailbreak attacks, {LATPC} framework achieves a superior balance between safety and utility. Moreover, our analysis underscores the effectiveness of extracting safety-critical dimensions from the latent space for constructing robust refusal feature attacks.},
	number = {{arXiv}:2501.10639},
	publisher = {{arXiv}},
	author = {Yi, Xin and Li, Yue and Wang, Linlin and Wang, Xiaoling and He, Liang},
	urldate = {2025-02-03},
	date = {2025-01-18},
	eprinttype = {arxiv},
	eprint = {2501.10639 [cs]},
	keywords = {language models, sequence modelling, robustness, interpretability, adversarial, jailbreaking},
	file = {Full Text PDF:/Users/ep/Zotero/storage/P74NKSHQ/Yi et al. - 2025 - Latent-space adversarial training with post-aware calibration for defending large language models ag.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FYA5TDRI/2501.html:text/html},
}

@misc{quach_conformal_2024,
	title = {Conformal Language Modeling},
	url = {http://arxiv.org/abs/2306.10193},
	doi = {10.48550/arXiv.2306.10193},
	abstract = {We propose a novel approach to conformal prediction for generative language models ({LMs}). Standard conformal prediction produces prediction sets -- in place of single predictions -- that have rigorous, statistical performance guarantees. {LM} responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the {LM} that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components -- such as phrases or sentences -- that are each independently correct (e.g., that are not "hallucinations"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different {LM} variants.},
	number = {{arXiv}:2306.10193},
	publisher = {{arXiv}},
	author = {Quach, Victor and Fisch, Adam and Schuster, Tal and Yala, Adam and Sohn, Jae Ho and Jaakkola, Tommi S. and Barzilay, Regina},
	urldate = {2025-05-12},
	date = {2024-06-01},
	eprinttype = {arxiv},
	eprint = {2306.10193 [cs]},
	keywords = {language models, uncertainty, conformal prediction},
	file = {Full Text PDF:/Users/ep/Zotero/storage/ZYFZJUHB/Quach et al. - 2024 - Conformal Language Modeling.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9IS6QWBI/2306.html:text/html},
}

@misc{davidov_calibrated_2025,
	title = {Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in {LLMs}},
	url = {http://arxiv.org/abs/2506.13593},
	doi = {10.48550/arXiv.2506.13593},
	abstract = {We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model ({LLM}) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned {LLMs}, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound ({LPB}) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the {LPB}, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative {AI} models.},
	number = {{arXiv}:2506.13593},
	publisher = {{arXiv}},
	author = {Davidov, Hen and Freidkin, Gilad and Feldman, Shai and Romano, Yaniv},
	urldate = {2025-07-14},
	date = {2025-06-20},
	eprinttype = {arxiv},
	eprint = {2506.13593 [cs]},
	keywords = {language models, probabilistic machine learning, conformal prediction, probability, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VR9MN6R5/Davidov et al. - 2025 - Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/PLSL9T8Y/2506.html:text/html},
}

@misc{fluri_perils_2025,
	title = {The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret},
	url = {http://arxiv.org/abs/2406.15753},
	doi = {10.48550/arXiv.2406.15753},
	shorttitle = {The Perils of Optimizing Learned Reward Functions},
	abstract = {In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as {RLHF}. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably.},
	number = {{arXiv}:2406.15753},
	publisher = {{arXiv}},
	author = {Fluri, Lukas and Lang, Leon and Abate, Alessandro and Forré, Patrick and Krueger, David and Skalse, Joar},
	urldate = {2025-07-15},
	date = {2025-07-08},
	eprinttype = {arxiv},
	eprint = {2406.15753 [cs]},
	keywords = {language models, reasoning, reward, uncertainty, test time compute},
	file = {Preprint PDF:/Users/ep/Zotero/storage/NFXGRJW2/Fluri et al. - 2025 - The Perils of Optimizing Learned Reward Functions Low Training Error Does Not Guarantee Low Regret.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XQLWXU8Y/2406.html:text/html},
}

@misc{buckman_importance_2020,
	title = {The Importance of Pessimism in Fixed-Dataset Policy Optimization},
	url = {http://arxiv.org/abs/2009.06799},
	doi = {10.48550/arXiv.2009.06799},
	abstract = {We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four {MinAtar} environments.},
	number = {{arXiv}:2009.06799},
	publisher = {{arXiv}},
	author = {Buckman, Jacob and Gelada, Carles and Bellemare, Marc G.},
	urldate = {2025-07-15},
	date = {2020-11-29},
	eprinttype = {arxiv},
	eprint = {2009.06799 [cs]},
	keywords = {language models, reinforcement learning, reward, uncertainty},
	file = {Preprint PDF:/Users/ep/Zotero/storage/BVIWUHUN/Buckman et al. - 2020 - The Importance of Pessimism in Fixed-Dataset Policy Optimization.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9SJZL2VV/2009.html:text/html},
}

@misc{correia_non-exchangeable_2025,
	title = {Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data},
	url = {http://arxiv.org/abs/2507.10425},
	doi = {10.48550/arXiv.2507.10425},
	shorttitle = {Non-exchangeable Conformal Prediction with Optimal Transport},
	abstract = {Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift.},
	number = {{arXiv}:2507.10425},
	publisher = {{arXiv}},
	author = {Correia, Alvaro H. C. and Louizos, Christos},
	urldate = {2025-07-17},
	date = {2025-07-14},
	eprinttype = {arxiv},
	eprint = {2507.10425 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, probability, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MQUKYQES/Correia and Louizos - 2025 - Non-exchangeable Conformal Prediction with Optimal Transport Tackling Distribution Shifts with Unla.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/KEA55T8B/2507.html:text/html},
}

@misc{guan_deliberative_2025,
	title = {Deliberative Alignment: Reasoning Enables Safer Language Models},
	url = {http://arxiv.org/abs/2412.16339},
	doi = {10.48550/arXiv.2412.16339},
	shorttitle = {Deliberative Alignment},
	abstract = {As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align {OpenAI}'s o-series models, and achieved highly precise adherence to {OpenAI}'s safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.},
	number = {{arXiv}:2412.16339},
	publisher = {{arXiv}},
	author = {Guan, Melody Y. and Joglekar, Manas and Wallace, Eric and Jain, Saachi and Barak, Boaz and Helyar, Alec and Dias, Rachel and Vallone, Andrea and Ren, Hongyu and Wei, Jason and Chung, Hyung Won and Toyer, Sam and Heidecke, Johannes and Beutel, Alex and Glaese, Amelia},
	urldate = {2025-07-22},
	date = {2025-01-08},
	eprinttype = {arxiv},
	eprint = {2412.16339 [cs]},
	keywords = {language models, reasoning, reinforcement learning, alignment},
	file = {Preprint PDF:/Users/ep/Zotero/storage/6QC5IJSH/Guan et al. - 2025 - Deliberative Alignment Reasoning Enables Safer Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FG2KQI4N/2412.html:text/html},
}

@misc{bailey_obfuscated_2025,
	title = {Obfuscated Activations Bypass {LLM} Latent-Space Defenses},
	url = {http://arxiv.org/abs/2412.09565},
	doi = {10.48550/arXiv.2412.09565},
	abstract = {Recent latent-space monitoring techniques have shown promise as defenses against {LLM} attacks. These defenses act as scanners that seek to detect harmful activations before they lead to undesirable actions. This prompts the question: Can models execute harmful behavior via inconspicuous latent states? Here, we study such obfuscated activations. We show that state-of-the-art latent-space defenses -- including sparse autoencoders, representation probing, and latent {OOD} detection -- are all vulnerable to obfuscated activations. For example, against probes trained to classify harmfulness, our attacks can often reduce recall from 100\% to 0\% while retaining a 90\% jailbreaking rate. However, obfuscation has limits: we find that on a complex task (writing {SQL} code), obfuscation reduces model performance. Together, our results demonstrate that neural activations are highly malleable: we can reshape activation patterns in a variety of ways, often while preserving a network's behavior. This poses a fundamental challenge to latent-space defenses.},
	number = {{arXiv}:2412.09565},
	author = {Bailey, Luke and Serrano, Alex and Sheshadri, Abhay and Seleznyov, Mikhail and Taylor, Jordan and Jenner, Erik and Hilton, Jacob and Casper, Stephen and Guestrin, Carlos and Emmons, Scott},
	urldate = {2025-08-17},
	date = {2025-02-08},
	eprinttype = {arxiv},
	eprint = {2412.09565 [cs]},
	keywords = {language models, robustness, interpretability, representation engineering, jailbreaking, anomaly detection},
	file = {Bailey et al. - 2025 - Obfuscated Activations Bypass LLM Latent-Space Defenses.pdf:/Users/ep/Zotero/storage/5X57AJX7/Bailey et al. - 2025 - Obfuscated Activations Bypass LLM Latent-Space Defenses.pdf:application/pdf},
}

@misc{kowal_its_2025,
	title = {It's the Thought that Counts: Evaluating the Attempts of Frontier {LLMs} to Persuade on Harmful Topics},
	url = {http://arxiv.org/abs/2506.02873},
	doi = {10.48550/arXiv.2506.02873},
	shorttitle = {It's the Thought that Counts},
	abstract = {Persuasion is a powerful capability of large language models ({LLMs}) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic {AI} systems. We propose the Attempt to Persuade Eval ({APE}) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier {LLMs} using a multi-turn conversational setup between simulated persuader and persuadee agents. {APE} explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of {LLM} risk. {APE} is available at github.com/{AlignmentResearch}/{AttemptPersuadeEval}},
	number = {{arXiv}:2506.02873},
	author = {Kowal, Matthew and Timm, Jasper and Godbout, Jean-Francois and Costello, Thomas and Arechar, Antonio A. and Pennycook, Gordon and Rand, David and Gleave, Adam and Pelrine, Kellin},
	urldate = {2025-08-16},
	date = {2025-06-03},
	eprinttype = {arxiv},
	eprint = {2506.02873 [cs]},
	keywords = {language models, evaluations, persuasion},
	file = {Kowal et al. - 2025 - It's the Thought that Counts Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics.pdf:/Users/ep/Zotero/storage/TE9GI2IE/Kowal et al. - 2025 - It's the Thought that Counts Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics.pdf:application/pdf},
}

@misc{golechha_among_2025,
	title = {Among Us: A Sandbox for Measuring and Detecting Agentic Deception},
	url = {http://arxiv.org/abs/2504.04072},
	doi = {10.48550/arXiv.2504.04072},
	shorttitle = {Among Us},
	abstract = {Prior studies on deception in language-based {AI} agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce \${\textbackslash}textit\{Among Us\}\$, a sandbox social deception game where {LLM}-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, \${\textbackslash}textit\{Among Us\}\$ can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate \$18\$ proprietary and open-weight {LLMs} and uncover a general trend: models trained with {RL} are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders ({SAEs}). We find that probes trained on a dataset of ``pretend you're a dishonest model: \${\textbackslash}dots\$'' generalize extremely well out-of-distribution, consistently obtaining {AUROCs} over 95\% even when evaluated just on the deceptive statement, without the chain of thought. We also find two {SAE} features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents.},
	number = {{arXiv}:2504.04072},
	author = {Golechha, Satvik and Garriga-Alonso, Adrià},
	urldate = {2025-08-16},
	date = {2025-05-16},
	eprinttype = {arxiv},
	eprint = {2504.04072 [cs]},
	file = {Golechha and Garriga-Alonso - 2025 - Among Us A Sandbox for Measuring and Detecting Agentic Deception.pdf:/Users/ep/Zotero/storage/3H8HGFPR/Golechha and Garriga-Alonso - 2025 - Among Us A Sandbox for Measuring and Detecting Agentic Deception.pdf:application/pdf},
}

@misc{mckenzie_stack_2025,
	title = {{STACK}: Adversarial Attacks on {LLM} Safeguard Pipelines},
	url = {http://arxiv.org/abs/2506.24068},
	doi = {10.48550/arXiv.2506.24068},
	shorttitle = {{STACK}},
	abstract = {Frontier {AI} developers are relying on layers of safeguards to protect against catastrophic misuse of {AI} systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google {DeepMind} and {OpenAI} pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model {ShieldGemma} across three attacks and two datasets, reducing the attack success rate ({ASR}) to 0\% on the catastrophic misuse dataset {ClearHarm}. Second, we introduce a {STaged} {AttaCK} ({STACK}) procedure that achieves 71\% {ASR} on {ClearHarm} in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate {STACK} in a transfer setting, achieving 33\% {ASR}, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.},
	number = {{arXiv}:2506.24068},
	author = {{McKenzie}, Ian R. and Hollinsworth, Oskar J. and Tseng, Tom and Davies, Xander and Casper, Stephen and Tucker, Aaron D. and Kirk, Robert and Gleave, Adam},
	urldate = {2025-08-16},
	date = {2025-07-18},
	eprinttype = {arxiv},
	eprint = {2506.24068 [cs]},
	keywords = {robustness, evaluations, adversarial},
	file = {McKenzie et al. - 2025 - STACK Adversarial Attacks on LLM Safeguard Pipelines.pdf:/Users/ep/Zotero/storage/PG95IPSM/McKenzie et al. - 2025 - STACK Adversarial Attacks on LLM Safeguard Pipelines.pdf:application/pdf},
}

@misc{cho_corrsteer_2025,
	title = {{CorrSteer}: Steering Improves Task Performance and Safety in {LLMs} through Correlation-based Sparse Autoencoder Feature Selection},
	url = {http://arxiv.org/abs/2508.12535},
	doi = {10.48550/arXiv.2508.12535},
	shorttitle = {{CorrSteer}},
	abstract = {Sparse Autoencoders ({SAEs}) can extract interpretable features from large language models ({LLMs}) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose {CorrSteer}, which selects features by correlating sample correctness with {SAE} activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on {QA}, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and {LLaMA} 3.1 8B, notably achieving a +4.1\% improvement in {MMLU} performance and a +22.9\% improvement in {HarmBench} with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated {SAE} steering across language model applications.},
	number = {{arXiv}:2508.12535},
	author = {Cho, Seonglae and Wu, Zekun and Koshiyama, Adriano},
	urldate = {2025-08-24},
	date = {2025-08-18},
	eprinttype = {arxiv},
	eprint = {2508.12535 [cs]},
	keywords = {language models, interpretability, representation engineering, safety guarantees},
	file = {Cho et al. - 2025 - CorrSteer Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Au.pdf:/Users/ep/Zotero/storage/THF5GK4V/Cho et al. - 2025 - CorrSteer Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Au.pdf:application/pdf},
}

@misc{goldowsky-dill_detecting_2025,
	title = {Detecting Strategic Deception Using Linear Probes},
	url = {http://arxiv.org/abs/2502.03407},
	doi = {10.48550/arXiv.2502.03407},
	abstract = {{AI} models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the {AI} might produce seemingly benign outputs while their internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations. We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al., 2023) and one of responses to simple roleplaying scenarios. We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively, such as concealing insider trading (Scheurer et al., 2023) and purposely underperforming on safety evaluations (Benton et al., 2024). We find that our probe distinguishes honest and deceptive responses with {AUROCs} between 0.96 and 0.999 on our evaluation datasets. If we set the decision threshold to have a 1\% false positive rate on chat data not related to deception, our probe catches 95-99\% of the deceptive responses. Overall we think white-box probes are promising for future monitoring systems, but current performance is insufficient as a robust defence against deception. Our probes' outputs can be viewed at data.apolloresearch.ai/dd and our code at github.com/{ApolloResearch}/deception-detection.},
	number = {{arXiv}:2502.03407},
	publisher = {{arXiv}},
	author = {Goldowsky-Dill, Nicholas and Chughtai, Bilal and Heimersheim, Stefan and Hobbhahn, Marius},
	urldate = {2025-08-26},
	date = {2025-02-05},
	eprinttype = {arxiv},
	eprint = {2502.03407 [cs]},
	keywords = {language models, interpretability, evaluations, deception, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/58BV8K83/Goldowsky-Dill et al. - 2025 - Detecting Strategic Deception Using Linear Probes.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/ZPMKSF5U/2502.html:text/html},
}

@misc{jeary_verifiably_2024,
	title = {Verifiably Robust Conformal Prediction},
	url = {http://arxiv.org/abs/2405.18942},
	doi = {10.48550/arXiv.2405.18942},
	abstract = {Conformal Prediction ({CP}) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, {CP}'s prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover {CP} guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support \${\textbackslash}ell{\textasciicircum}2\$-bounded perturbations and classification tasks. This paper introduces {VRCP} (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our {VRCP} method is the first to support perturbations bounded by arbitrary norms including \${\textbackslash}ell{\textasciicircum}1\$, \${\textbackslash}ell{\textasciicircum}2\$, and \${\textbackslash}ell{\textasciicircum}{\textbackslash}infty\$, as well as regression tasks. We evaluate and compare our approach on image classification tasks ({CIFAR}10, {CIFAR}100, and {TinyImageNet}) and regression tasks for deep reinforcement learning environments. In every case, {VRCP} achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the {SotA}.},
	number = {{arXiv}:2405.18942},
	publisher = {{arXiv}},
	author = {Jeary, Linus and Kuipers, Tom and Hosseini, Mehran and Paoletti, Nicola},
	urldate = {2025-08-27},
	date = {2024-11-16},
	eprinttype = {arxiv},
	eprint = {2405.18942 [cs]},
	keywords = {robustness, adversarial, probabilistic machine learning, conformal prediction, verification},
	file = {Preprint PDF:/Users/ep/Zotero/storage/PKF6HN5J/Jeary et al. - 2024 - Verifiably Robust Conformal Prediction.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/3KNEVDAA/2405.html:text/html},
}

@misc{kandpal_backdoor_2023,
	title = {Backdoor Attacks for In-Context Learning with Language Models},
	url = {http://arxiv.org/abs/2307.14692},
	doi = {10.48550/arXiv.2307.14692},
	abstract = {Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model {APIs}. This consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. Finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.},
	number = {{arXiv}:2307.14692},
	publisher = {{arXiv}},
	author = {Kandpal, Nikhil and Jagielski, Matthew and Tramèr, Florian and Carlini, Nicholas},
	urldate = {2025-09-03},
	date = {2023-07-27},
	eprinttype = {arxiv},
	eprint = {2307.14692 [cs]},
	keywords = {language models, adversarial, backdoor},
	file = {Preprint PDF:/Users/ep/Zotero/storage/9VCJ8LW7/Kandpal et al. - 2023 - Backdoor Attacks for In-Context Learning with Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/IIS78SSK/2307.html:text/html},
}

@misc{mazeika_harmbench_2024,
	title = {{HarmBench}: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
	url = {http://arxiv.org/abs/2402.04249},
	doi = {10.48550/arXiv.2402.04249},
	shorttitle = {{HarmBench}},
	abstract = {Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models ({LLMs}), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce {HarmBench}, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design {HarmBench} to meet these criteria. Using {HarmBench}, we conduct a large-scale comparison of 18 red teaming methods and 33 target {LLMs} and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances {LLM} robustness across a wide range of attacks, demonstrating how {HarmBench} enables codevelopment of attacks and defenses. We open source {HarmBench} at https://github.com/centerforaisafety/{HarmBench}.},
	number = {{arXiv}:2402.04249},
	publisher = {{arXiv}},
	author = {Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan},
	urldate = {2025-09-03},
	date = {2024-02-27},
	eprinttype = {arxiv},
	eprint = {2402.04249 [cs]},
	keywords = {language models, evaluations, agents, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/T8G8XI6G/Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/8D2HXMNE/2402.html:text/html},
}

@misc{lindner_learning_2024,
	title = {Learning Safety Constraints from Demonstrations with Unknown Rewards},
	url = {http://arxiv.org/abs/2305.16147},
	doi = {10.48550/arXiv.2305.16147},
	abstract = {We propose Convex Constraint Learning for Reinforcement Learning ({CoCoRL}), a novel approach for inferring shared constraints in a Constrained Markov Decision Process ({CMDP}) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, {CoCoRL} can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. {CoCoRL} constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, {CoCoRL} converges to the true safe set with no policy regret. We evaluate {CoCoRL} in gridworld environments and a driving simulation with multiple constraints. {CoCoRL} learns constraints that lead to safe driving behavior. Importantly, we can safely transfer the learned constraints to different tasks and environments. In contrast, alternative methods based on Inverse Reinforcement Learning ({IRL}) often exhibit poor performance and learn unsafe policies.},
	number = {{arXiv}:2305.16147},
	publisher = {{arXiv}},
	author = {Lindner, David and Chen, Xin and Tschiatschek, Sebastian and Hofmann, Katja and Krause, Andreas},
	urldate = {2025-09-08},
	date = {2024-03-02},
	eprinttype = {arxiv},
	eprint = {2305.16147 [cs]},
	keywords = {sequential decision making, reinforcement learning, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/66F2CTKX/Lindner et al. - 2024 - Learning Safety Constraints from Demonstrations with Unknown Rewards.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/3INRK59E/2305.html:text/html},
}

@misc{novello_out--distribution_2024,
	title = {Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)},
	url = {http://arxiv.org/abs/2403.11532},
	doi = {10.48550/arXiv.2403.11532},
	shorttitle = {Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?},
	abstract = {Research on Out-Of-Distribution ({OOD}) detection focuses mainly on building scores that efficiently distinguish {OOD} data from In Distribution ({ID}) data. On the other hand, Conformal Prediction ({CP}) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use {CP} to better assess the efficiency of {OOD} scores. Specifically, we emphasize that in standard {OOD} benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal {AUROC} and conformal {FRP}@{TPR}95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference {OOD} and anomaly detection benchmarks, {OpenOOD} (Yang et al., 2022) and {ADBench} (Han et al., 2022). We also show that the benefits of using {OOD} together with {CP} apply the other way around by using {OOD} scores as non-conformity scores, which results in improving upon current {CP} methods. One of the key messages of these contributions is that since {OOD} is concerned with designing scores and {CP} with interpreting these scores, the two fields may be inherently intertwined.},
	number = {{arXiv}:2403.11532},
	publisher = {{arXiv}},
	author = {Novello, Paul and Dalmau, Joseba and Andeol, Léo},
	urldate = {2025-09-08},
	date = {2024-03-18},
	eprinttype = {arxiv},
	eprint = {2403.11532 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, probability, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/X8E6S4BH/Novello et al. - 2024 - Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa).pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/L5PUUK4P/2403.html:text/html},
}

@misc{kaur_codit_2022,
	title = {{CODiT}: Conformal Out-of-Distribution Detection in Time-Series Data},
	url = {http://arxiv.org/abs/2207.11769},
	doi = {10.48550/arXiv.2207.11769},
	shorttitle = {{CODiT}},
	abstract = {Machine learning models are prone to making incorrect predictions on inputs that are far from the training distribution. This hinders their deployment in safety-critical applications such as autonomous vehicles and healthcare. The detection of a shift from the training distribution of individual datapoints has gained attention. A number of techniques have been proposed for such out-of-distribution ({OOD}) detection. But in many applications, the inputs to a machine learning model form a temporal sequence. Existing techniques for {OOD} detection in time-series data either do not exploit temporal relationships in the sequence or do not provide any guarantees on detection. We propose using deviation from the in-distribution temporal equivariance as the non-conformity measure in conformal anomaly detection framework for {OOD} detection in time-series data.Computing independent predictions from multiple conformal detectors based on the proposed measure and combining these predictions by Fisher's method leads to the proposed detector {CODiT} with guarantees on false detection in time-series data. We illustrate the efficacy of {CODiT} by achieving state-of-the-art results on computer vision datasets in autonomous driving. We also show that {CODiT} can be used for {OOD} detection in non-vision datasets by performing experiments on the physiological {GAIT} sensory dataset. Code, data, and trained models are available at https://github.com/kaustubhsridhar/time-series-{OOD}.},
	number = {{arXiv}:2207.11769},
	publisher = {{arXiv}},
	author = {Kaur, Ramneet and Sridhar, Kaustubh and Park, Sangdon and Jha, Susmit and Roy, Anirban and Sokolsky, Oleg and Lee, Insup},
	urldate = {2025-09-08},
	date = {2022-07-24},
	eprinttype = {arxiv},
	eprint = {2207.11769 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, probability, anomaly detection},
	file = {Snapshot:/Users/ep/Zotero/storage/LW7XFNXP/2207.html:text/html},
}

@inproceedings{hennhofer_leave-one-out-_2024,
	title = {Leave-One-Out-, Bootstrap- and Cross-Conformal Anomaly Detectors},
	url = {http://arxiv.org/abs/2402.16388},
	doi = {10.1109/ICKG63256.2024.00022},
	abstract = {The requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates (\${\textbackslash}alpha\$) without compromising the statistical power (\$1-{\textbackslash}beta\$) of these systems can build trust and reduce costs related to false discoveries. The field of conformal anomaly detection emerges as a promising approach for providing respective statistical guarantees by model calibration. However, the dependency on calibration data poses practical limitations - especially within low-data regimes. In this work, we formally define and evaluate leave-one-out-, bootstrap-, and cross-conformal methods for anomaly detection, incrementing on methods from the field of conformal prediction. Looking beyond the classical inductive conformal anomaly detection, we demonstrate that derived methods for calculating resampling-conformal \$p\$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) as they make more efficient use of available data. We validate derived methods and quantify their improvements for a range of one-class classifiers and datasets.},
	pages = {110--119},
	booktitle = {2024 {IEEE} International Conference on Knowledge Graph ({ICKG})},
	author = {Hennhöfer, Oliver and Preisach, Christine},
	urldate = {2025-09-09},
	date = {2024-12-11},
	eprinttype = {arxiv},
	eprint = {2402.16388 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, probability, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/EXMEA6RH/Hennhöfer and Preisach - 2024 - Leave-One-Out-, Bootstrap- and Cross-Conformal Anomaly Detectors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/YDABAI72/2402.html:text/html},
}

@misc{adams_conformal_2025,
	title = {Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics},
	url = {http://arxiv.org/abs/2504.01172},
	doi = {10.48550/arXiv.2504.01172},
	abstract = {This paper considers the problem of outlier detection in functional data analysis focusing particularly on the more difficult case of shape outliers. We present an inductive conformal anomaly detection method based on elastic functional distance metrics. This method is evaluated and compared to similar conformal anomaly detection methods for functional data using simulation experiments. The method is also used in the analysis of two real exemplar data sets that show its utility in practical applications. The results demonstrate the efficacy of the proposed method for detecting both magnitude and shape outliers in two distinct outlier detection scenarios.},
	number = {{arXiv}:2504.01172},
	publisher = {{arXiv}},
	author = {Adams, Jason and Berman, Brandon and Michalenko, Joshua and Tucker, J. Derek},
	urldate = {2025-09-09},
	date = {2025-04-10},
	eprinttype = {arxiv},
	eprint = {2504.01172 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/ZY8T6HIN/Adams et al. - 2025 - Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/UGTS8AAU/2504.html:text/html},
}

@misc{hindy_diagnostic_2024,
	title = {Diagnostic Runtime Monitoring with Martingales},
	url = {http://arxiv.org/abs/2407.21748},
	doi = {10.48550/arXiv.2407.21748},
	abstract = {Machine learning systems deployed in safety-critical robotics settings must be robust to distribution shifts. However, system designers must understand the cause of a distribution shift in order to implement the appropriate intervention or mitigation strategy and prevent system failure. In this paper, we present a novel framework for diagnosing distribution shifts in a streaming fashion by deploying multiple stochastic martingales simultaneously. We show that knowledge of the underlying cause of a distribution shift can lead to proper interventions over the lifecycle of a deployed system. Our experimental framework can easily be adapted to different types of distribution shifts, models, and datasets. We find that our method outperforms existing work on diagnosing distribution shifts in terms of speed, accuracy, and flexibility, and validate the efficiency of our model in both simulated and live hardware settings.},
	number = {{arXiv}:2407.21748},
	publisher = {{arXiv}},
	author = {Hindy, Ali and Luo, Rachel and Banerjee, Somrita and Kuck, Jonathan and Schmerling, Edward and Pavone, Marco},
	urldate = {2025-09-09},
	date = {2024-07-31},
	eprinttype = {arxiv},
	eprint = {2407.21748 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/UEBDY2CB/Hindy et al. - 2024 - Diagnostic Runtime Monitoring with Martingales.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/6X8AZ8ZK/2407.html:text/html},
}

@misc{piet_jailbreaksovertime_2025,
	title = {{JailbreaksOverTime}: Detecting Jailbreak Attacks Under Distribution Shift},
	url = {http://arxiv.org/abs/2504.19440},
	doi = {10.48550/arXiv.2504.19440},
	shorttitle = {{JailbreaksOverTime}},
	abstract = {Safety and security remain critical concerns in {AI} deployment. Despite safety training through reinforcement learning with human feedback ({RLHF}) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release {JailbreaksOverTime}, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4\% to 0.3\% at a false positive rate of 0.1\%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1\%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach.},
	number = {{arXiv}:2504.19440},
	publisher = {{arXiv}},
	author = {Piet, Julien and Huang, Xiao and Jacob, Dennis and Chow, Annabella and Alrashed, Maha and Zhao, Geng and Hu, Zhanhao and Sitawarin, Chawin and Alomair, Basel and Wagner, David},
	urldate = {2025-09-10},
	date = {2025-04-28},
	eprinttype = {arxiv},
	eprint = {2504.19440 [cs]},
	keywords = {language models, robustness, adversarial, jailbreaking, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/TZFYVB7I/Piet et al. - 2025 - JailbreaksOverTime Detecting Jailbreak Attacks Under Distribution Shift.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/JKMBFL2X/2504.html:text/html},
}

@misc{luo_online_2024,
	title = {Online Distribution Shift Detection via Recency Prediction},
	url = {http://arxiv.org/abs/2211.09916},
	doi = {10.48550/arXiv.2211.09916},
	abstract = {When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate - i.e., when there is no distribution shift, our system is very unlikely (with probability \${\textless} {\textbackslash}epsilon\$) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our approach in both simulation and hardware for a visual servoing task, and show that our method indeed issues an alert before a failure occurs.},
	number = {{arXiv}:2211.09916},
	publisher = {{arXiv}},
	author = {Luo, Rachel and Sinha, Rohan and Sun, Yixiao and Hindy, Ali and Zhao, Shengjia and Savarese, Silvio and Schmerling, Edward and Pavone, Marco},
	urldate = {2025-09-12},
	date = {2024-05-18},
	eprinttype = {arxiv},
	eprint = {2211.09916 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/C3VW3GYC/Luo et al. - 2024 - Online Distribution Shift Detection via Recency Prediction.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XCFLCIIR/2211.html:text/html},
}

@misc{zou_universal_2023,
	title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
	url = {http://arxiv.org/abs/2307.15043},
	doi = {10.48550/arXiv.2307.15043},
	abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against {LLMs} -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an {LLM} to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released {LLMs}. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to {ChatGPT}, Bard, and Claude, as well as open source {LLMs} such as {LLaMA}-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
	number = {{arXiv}:2307.15043},
	publisher = {{arXiv}},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	urldate = {2025-09-16},
	date = {2023-12-20},
	eprinttype = {arxiv},
	eprint = {2307.15043 [cs]},
	keywords = {language models, alignment, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/52S86J9E/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on Aligned Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/KEZPLT6C/2307.html:text/html},
}

@misc{wen_language_2024,
	title = {Language Models Learn to Mislead Humans via {RLHF}},
	url = {http://arxiv.org/abs/2409.12822},
	doi = {10.48550/arXiv.2409.12822},
	abstract = {Language models ({LMs}) can produce errors that are hard to detect for humans, especially when the task is complex. {RLHF}, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, {LMs} might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard {RLHF} pipeline, calling it "U-{SOPHISTRY}" since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task ({QuALITY}) and programming task ({APPS}), {RLHF} makes {LMs} better at convincing our subjects but not at completing the task correctly. {RLHF} also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1\% on {QuALITY} and 18.3\% on {APPS}. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored {LMs}), does not generalize to U-{SOPHISTRY}. Our results highlight an important failure mode of {RLHF} and call for more research in assisting humans to align them.},
	number = {{arXiv}:2409.12822},
	publisher = {{arXiv}},
	author = {Wen, Jiaxin and Zhong, Ruiqi and Khan, Akbir and Perez, Ethan and Steinhardt, Jacob and Huang, Minlie and Bowman, Samuel R. and He, He and Feng, Shi},
	urldate = {2025-09-16},
	date = {2024-12-08},
	eprinttype = {arxiv},
	eprint = {2409.12822 [cs]},
	keywords = {language models, reinforcement learning, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VTWX5U5G/Wen et al. - 2024 - Language Models Learn to Mislead Humans via RLHF.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/LIS65M6I/2409.html:text/html},
}

@misc{ward_honesty_2023,
	title = {Honesty Is the Best Policy: Defining and Mitigating {AI} Deception},
	url = {http://arxiv.org/abs/2312.01350},
	doi = {10.48550/arXiv.2312.01350},
	shorttitle = {Honesty Is the Best Policy},
	abstract = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of {AI} systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic {AI}, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.},
	number = {{arXiv}:2312.01350},
	publisher = {{arXiv}},
	author = {Ward, Francis Rhys and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
	urldate = {2025-09-17},
	date = {2023-12-03},
	eprinttype = {arxiv},
	eprint = {2312.01350 [cs]},
	keywords = {language models, interpretability, causality, deception, intentionality},
	file = {Preprint PDF:/Users/ep/Zotero/storage/8XZ9DPL3/Ward et al. - 2023 - Honesty Is the Best Policy Defining and Mitigating AI Deception.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/SRX5G5DM/2312.html:text/html},
}

@misc{ward_reasons_2024,
	title = {The Reasons that Agents Act: Intention and Instrumental Goals},
	url = {http://arxiv.org/abs/2402.07221},
	doi = {10.48550/arXiv.2402.07221},
	shorttitle = {The Reasons that Agents Act},
	abstract = {Intention is an important and challenging concept in {AI}. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to {AI} systems is contentious, and there is no universally accepted theory of intention applicable to {AI} agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe {AI} agents. Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour.},
	number = {{arXiv}:2402.07221},
	publisher = {{arXiv}},
	author = {Ward, Francis Rhys and {MacDermott}, Matt and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
	urldate = {2025-09-17},
	date = {2024-02-15},
	eprinttype = {arxiv},
	eprint = {2402.07221 [cs]},
	keywords = {language models, interpretability, deception, intentionality},
	file = {Preprint PDF:/Users/ep/Zotero/storage/9QAHI42F/Ward et al. - 2024 - The Reasons that Agents Act Intention and Instrumental Goals.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/EUV8N6KY/2402.html:text/html},
}

@misc{gupta_rl-obfuscation_2025,
	title = {{RL}-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?},
	url = {http://arxiv.org/abs/2506.14261},
	doi = {10.48550/arXiv.2506.14261},
	shorttitle = {{RL}-Obfuscation},
	abstract = {Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can {LLMs} learn to evade such monitors? To study this, we introduce {RL}-Obfuscation, in which {LLMs} are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply {RL}-Obfuscation to {LLMs} ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by {RL} bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.},
	number = {{arXiv}:2506.14261},
	publisher = {{arXiv}},
	author = {Gupta, Rohan and Jenner, Erik},
	urldate = {2025-09-17},
	date = {2025-06-18},
	eprinttype = {arxiv},
	eprint = {2506.14261 [cs]},
	keywords = {language models, interpretability, reinforcement learning, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/3J6Q448H/Gupta and Jenner - 2025 - RL-Obfuscation Can Language Models Learn to Evade Latent-Space Monitors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/62VX8247/2506.html:text/html},
}

@misc{peng_copen_2022,
	title = {{COPEN}: Probing Conceptual Knowledge in Pre-trained Language Models},
	url = {http://arxiv.org/abs/2211.04079},
	doi = {10.48550/arXiv.2211.04079},
	shorttitle = {{COPEN}},
	abstract = {Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models ({PLMs}) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of {PLMs} by designing three tasks to probe whether {PLMs} organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is {COPEN}, a {COnceptual} knowledge Probing {bENchmark}. Extensive experiments on different sizes and types of {PLMs} show that existing {PLMs} systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in {PLMs}. {COPEN} and our codes are publicly released at https://github.com/{THU}-{KEG}/{COPEN}.},
	number = {{arXiv}:2211.04079},
	publisher = {{arXiv}},
	author = {Peng, Hao and Wang, Xiaozhi and Hu, Shengding and Jin, Hailong and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Liu, Qun},
	urldate = {2025-09-22},
	date = {2022-11-08},
	eprinttype = {arxiv},
	eprint = {2211.04079 [cs]},
	keywords = {language models, interpretability, representation engineering},
	file = {Preprint PDF:/Users/ep/Zotero/storage/Y784J632/Peng et al. - 2022 - COPEN Probing Conceptual Knowledge in Pre-trained Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/7L6F5JYU/2211.html:text/html},
}

@misc{chan_can_2025,
	title = {Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models},
	url = {http://arxiv.org/abs/2507.12428},
	doi = {10.48550/arXiv.2507.12428},
	shorttitle = {Can We Predict Alignment Before Models Finish Thinking?},
	abstract = {Open-weights reasoning language models generate long chains-of-thought ({CoTs}) before producing a final response, which improves performance but introduces additional alignment risks, with harmful content often appearing in both the {CoTs} and the final outputs. In this work, we investigate if we can use {CoTs} to predict final response misalignment. We evaluate a range of monitoring approaches, including humans, highly-capable large language models, and text classifiers, using either {CoT} text or activations. First, we find that a simple linear probe trained on {CoT} activations can significantly outperform all text-based methods in predicting whether a final response will be safe or unsafe. {CoT} texts are often unfaithful and can mislead humans and classifiers, while model latents (i.e., {CoT} activations) offer a more reliable predictive signal. Second, the probe makes accurate predictions before reasoning completes, achieving strong performance even when applied to early {CoT} segments. These findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.},
	number = {{arXiv}:2507.12428},
	publisher = {{arXiv}},
	author = {Chan, Yik Siu and Yong, Zheng-Xin and Bach, Stephen H.},
	urldate = {2025-09-22},
	date = {2025-07-16},
	eprinttype = {arxiv},
	eprint = {2507.12428 [cs]},
	keywords = {language models, interpretability, reasoning, representation engineering, alignment, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MR2J94RS/Chan et al. - 2025 - Can We Predict Alignment Before Models Finish Thinking Towards Monitoring Misaligned Reasoning Mode.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/VJYGHK3E/2507.html:text/html},
}

@misc{wang_when_2025,
	title = {When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models},
	url = {http://arxiv.org/abs/2508.02087},
	doi = {10.48550/arXiv.2508.02087},
	shorttitle = {When Truth Is Overridden},
	abstract = {Large Language Models ({LLMs}) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within {LLMs}. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful {AI} systems.},
	number = {{arXiv}:2508.02087},
	publisher = {{arXiv}},
	author = {Wang, Keyu and Li, Jin and Yang, Shu and Zhang, Zhuoran and Wang, Di},
	urldate = {2025-09-22},
	date = {2025-08-05},
	eprinttype = {arxiv},
	eprint = {2508.02087 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/3LBVGTTV/Wang et al. - 2025 - When Truth Is Overridden Uncovering the Internal Origins of Sycophancy in Large Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/4HPTMKMA/2508.html:text/html},
}

@misc{wang_when_2025-1,
	title = {When Thinking {LLMs} Lie: Unveiling the Strategic Deception in Representations of Reasoning Models},
	url = {http://arxiv.org/abs/2506.04909},
	doi = {10.48550/arXiv.2506.04909},
	shorttitle = {When Thinking {LLMs} Lie},
	abstract = {The honesty of large language models ({LLMs}) is a critical alignment challenge, especially as advanced systems with chain-of-thought ({CoT}) reasoning may strategically deceive humans. Unlike traditional honesty issues on {LLMs}, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in {CoT}-enabled {LLMs}, extracting "deception vectors" via Linear Artificial Tomography ({LAT}) for 89\% detection accuracy. Through activation steering, we achieve a 40\% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy {AI} alignment.},
	number = {{arXiv}:2506.04909},
	publisher = {{arXiv}},
	author = {Wang, Kai and Zhang, Yihao and Sun, Meng},
	urldate = {2025-09-22},
	date = {2025-06-05},
	eprinttype = {arxiv},
	eprint = {2506.04909 [cs]},
	keywords = {language models, interpretability, reasoning, representation engineering, alignment, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MHGXEFVD/Wang et al. - 2025 - When Thinking LLMs Lie Unveiling the Strategic Deception in Representations of Reasoning Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/AWH79BSX/2506.html:text/html},
}

@misc{zhong_watch_2025,
	title = {Watch the Weights: Unsupervised monitoring and control of fine-tuned {LLMs}},
	url = {http://arxiv.org/abs/2508.00161},
	doi = {10.48550/arXiv.2508.00161},
	shorttitle = {Watch the Weights},
	abstract = {The releases of powerful open-weight large language models ({LLMs}) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution. In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned {LLMs} that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision. For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100\% of attacks with a false positive rate below 1.2\%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42\% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models ({OLMo}, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation. Our implementation can be found at https://github.com/fjzzq2002/{WeightWatch}.},
	number = {{arXiv}:2508.00161},
	publisher = {{arXiv}},
	author = {Zhong, Ziqian and Raghunathan, Aditi},
	urldate = {2025-09-22},
	date = {2025-07-31},
	eprinttype = {arxiv},
	eprint = {2508.00161 [cs]},
	keywords = {language models, interpretability, representation engineering, evaluations, alignment, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/P42J2D36/Zhong and Raghunathan - 2025 - Watch the Weights Unsupervised monitoring and control of fine-tuned LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/S6YTFSZK/2508.html:text/html},
}

@misc{beaglehole_toward_2025,
	title = {Toward universal steering and monitoring of {AI} models},
	url = {http://arxiv.org/abs/2502.03708},
	doi = {10.48550/arXiv.2502.03708},
	abstract = {Modern {AI} models contain much of human knowledge, yet understanding of their internal representation of this knowledge remains elusive. Characterizing the structure and properties of this representation will lead to improvements in model capabilities and development of effective safeguards. Building on recent advances in feature learning, we develop an effective, scalable approach for extracting linear representations of general concepts in large-scale {AI} models (language models, vision-language models, and reasoning models). We show how these representations enable model steering, through which we expose vulnerabilities, mitigate misaligned behaviors, and improve model capabilities. Additionally, we demonstrate that concept representations are remarkably transferable across human languages and combinable to enable multi-concept steering. Through quantitative analysis across hundreds of concepts, we find that newer, larger models are more steerable and steering can improve model capabilities beyond standard prompting. We show how concept representations are effective for monitoring misaligned content (hallucinations, toxic content). We demonstrate that predictive models built using concept representations are more accurate for monitoring misaligned content than using models that judge outputs directly. Together, our results illustrate the power of using internal representations to map the knowledge in {AI} models, advance {AI} safety, and improve model capabilities.},
	number = {{arXiv}:2502.03708},
	publisher = {{arXiv}},
	author = {Beaglehole, Daniel and Radhakrishnan, Adityanarayanan and Boix-Adserà, Enric and Belkin, Mikhail},
	urldate = {2025-09-22},
	date = {2025-05-28},
	eprinttype = {arxiv},
	eprint = {2502.03708 [cs]},
	keywords = {language models, interpretability, representation engineering, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/XTFW9DPA/Beaglehole et al. - 2025 - Toward universal steering and monitoring of AI models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9S89PI8I/2502.html:text/html},
}

@misc{chaudhary_safetynet_2025,
	title = {{SafetyNet}: Detecting Harmful Outputs in {LLMs} by Modeling and Monitoring Deceptive Behaviors},
	url = {http://arxiv.org/abs/2505.14300},
	doi = {10.48550/arXiv.2505.14300},
	shorttitle = {{SafetyNet}},
	abstract = {High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models ({LLMs}) need monitoring safeguards. We propose a real-time framework to predict harmful {AI} outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether {LLMs} display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96\% accuracy in detecting harmful cases using our unsupervised ensemble approach.},
	number = {{arXiv}:2505.14300},
	publisher = {{arXiv}},
	author = {Chaudhary, Maheep and Barez, Fazl},
	urldate = {2025-09-22},
	date = {2025-05-20},
	eprinttype = {arxiv},
	eprint = {2505.14300 [cs]},
	keywords = {language models, interpretability, representation engineering, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/FHF3D5HN/Chaudhary and Barez - 2025 - SafetyNet Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/AX6QX9WS/2505.html:text/html},
}

@misc{clymer_poser_2024,
	title = {Poser: Unmasking Alignment Faking {LLMs} by Manipulating Their Internals},
	url = {http://arxiv.org/abs/2405.05466},
	doi = {10.48550/arXiv.2405.05466},
	shorttitle = {Poser},
	abstract = {Like a criminal under investigation, Large Language Models ({LLMs}) might pretend to be aligned while evaluated and misbehave when they have a good opportunity. Can current interpretability methods catch these 'alignment fakers?' To answer this question, we introduce a benchmark that consists of 324 pairs of {LLMs} fine-tuned to select actions in role-play scenarios. One model in each pair is consistently benign (aligned). The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking). The task is to identify the alignment faking model using only inputs where the two models behave identically. We test five detection strategies, one of which identifies 98\% of alignment-fakers.},
	number = {{arXiv}:2405.05466},
	publisher = {{arXiv}},
	author = {Clymer, Joshua and Juang, Caden and Field, Severin},
	urldate = {2025-09-22},
	date = {2024-05-11},
	eprinttype = {arxiv},
	eprint = {2405.05466 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/Y8CLDHWV/Clymer et al. - 2024 - Poser Unmasking Alignment Faking LLMs by Manipulating Their Internals.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/2JMG7PHC/2405.html:text/html},
}

@misc{xie_sorry-bench_2025,
	title = {{SORRY}-Bench: Systematically Evaluating Large Language Model Safety Refusal},
	url = {http://arxiv.org/abs/2406.14598},
	doi = {10.48550/arXiv.2406.14598},
	shorttitle = {{SORRY}-Bench},
	abstract = {Evaluating aligned large language models' ({LLMs}) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with {SORRY}-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. {SORRY}-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement {SORRY}-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large {LLMs} (e.g., {GPT}-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse {LLM}-as-a-judge designs, we show that fine-tuned 7B {LLMs} can achieve accuracy comparable to {GPT}-4 scale {LLMs}, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight {LLMs} on {SORRY}-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of {LLMs}' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through https://sorry-bench.github.io.},
	number = {{arXiv}:2406.14598},
	publisher = {{arXiv}},
	author = {Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and Jia, Ruoxi and Li, Bo and Li, Kai and Chen, Danqi and Henderson, Peter and Mittal, Prateek},
	urldate = {2025-09-22},
	date = {2025-03-01},
	eprinttype = {arxiv},
	eprint = {2406.14598 [cs]},
	keywords = {language models, reasoning, evaluations, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/RRVHBCAG/Xie et al. - 2025 - SORRY-Bench Systematically Evaluating Large Language Model Safety Refusal.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/68YVCKI7/2406.html:text/html},
}

@misc{wu_opendeception_2025,
	title = {{OpenDeception}: Benchmarking and Investigating {AI} Deceptive Behaviors via Open-ended Interaction Simulation},
	url = {http://arxiv.org/abs/2504.13707},
	doi = {10.48550/arXiv.2504.13707},
	shorttitle = {{OpenDeception}},
	abstract = {As the general capabilities of large language models ({LLMs}) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce {OpenDeception}, a novel deception evaluation framework with an open-ended scenario dataset. {OpenDeception} jointly evaluates both the deception intention and capabilities of {LLM}-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where {LLMs} intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream {LLMs} on {OpenDeception} highlights the urgent need to address deception risks and security concerns in {LLM}-based agents: the deception intention ratio across the models exceeds 80\%, while the deception success rate surpasses 50\%. Furthermore, we observe that {LLMs} with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.},
	number = {{arXiv}:2504.13707},
	publisher = {{arXiv}},
	author = {Wu, Yichen and Pan, Xudong and Hong, Geng and Yang, Min},
	urldate = {2025-09-22},
	date = {2025-09-08},
	eprinttype = {arxiv},
	eprint = {2504.13707 [cs]},
	keywords = {language models, evaluations, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/7X85I4IM/Wu et al. - 2025 - OpenDeception Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simu.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/JINYR6G6/2504.html:text/html},
}

@misc{ji_mitigating_2025,
	title = {Mitigating Deceptive Alignment via Self-Monitoring},
	url = {http://arxiv.org/abs/2505.18807},
	doi = {10.48550/arXiv.2505.18807},
	abstract = {Modern large language models rely on chain-of-thought ({CoT}) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the {CoT} process itself, named {CoT} Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce {DeceptionBench}, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various {LLMs} and show that unrestricted {CoT} roughly aggravates the deceptive tendency. In contrast, {CoT} Monitor+ cuts deceptive behaviors by 43.8\% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in {RL} fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io},
	number = {{arXiv}:2505.18807},
	publisher = {{arXiv}},
	author = {Ji, Jiaming and Chen, Wenqi and Wang, Kaile and Hong, Donghai and Fang, Sitong and Chen, Boyuan and Zhou, Jiayi and Dai, Juntao and Han, Sirui and Guo, Yike and Yang, Yaodong},
	urldate = {2025-09-22},
	date = {2025-05-24},
	eprinttype = {arxiv},
	eprint = {2505.18807 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VDRBA49V/Ji et al. - 2025 - Mitigating Deceptive Alignment via Self-Monitoring.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9JQKHIW8/2505.html:text/html},
}

@misc{kran_darkbench_2025,
	title = {{DarkBench}: Benchmarking Dark Patterns in Large Language Models},
	url = {http://arxiv.org/abs/2503.10728},
	doi = {10.48550/arXiv.2503.10728},
	shorttitle = {{DarkBench}},
	abstract = {We introduce {DarkBench}, a comprehensive benchmark for detecting dark design patterns--manipulative techniques that influence user behavior--in interactions with large language models ({LLMs}). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies ({OpenAI}, Anthropic, Meta, Mistral, Google) and find that some {LLMs} are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing {LLMs} should recognize and mitigate the impact of dark design patterns to promote more ethical {AI}.},
	number = {{arXiv}:2503.10728},
	publisher = {{arXiv}},
	author = {Kran, Esben and Nguyen, Hieu Minh "Jord" and Kundu, Akash and Jawhar, Sami and Park, Jinsuk and Jurewicz, Mateusz Maria},
	urldate = {2025-09-22},
	date = {2025-03-13},
	eprinttype = {arxiv},
	eprint = {2503.10728 [cs]},
	keywords = {evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/5GTZF4B4/Kran et al. - 2025 - DarkBench Benchmarking Dark Patterns in Large Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/LNR3F54H/2503.html:text/html},
}

@misc{he_evaluating_2025,
	title = {Evaluating the Paperclip Maximizer: Are {RL}-Based Language Models More Likely to Pursue Instrumental Goals?},
	url = {http://arxiv.org/abs/2502.12206},
	doi = {10.48550/arXiv.2502.12206},
	shorttitle = {Evaluating the Paperclip Maximizer},
	abstract = {As large language models ({LLMs}) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is {\textbackslash}textit\{instrumental convergence\}, where an {AI} system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning ({RL})-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in {LLMs} by comparing models trained with direct {RL} optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback ({RLHF}). We hypothesize that {RL}-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce {InstrumentalEval}, a benchmark for evaluating instrumental convergence in {RL}-trained {LLMs}. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence. Our findings contribute to a deeper understanding of alignment challenges in {AI} systems and the risks posed by unintended model behaviors.},
	number = {{arXiv}:2502.12206},
	publisher = {{arXiv}},
	author = {He, Yufei and Li, Yuexin and Wu, Jiaying and Sui, Yuan and Chen, Yulin and Hooi, Bryan},
	urldate = {2025-09-22},
	date = {2025-02-16},
	eprinttype = {arxiv},
	eprint = {2502.12206 [cs]},
	keywords = {language models, reasoning, reinforcement learning, evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/SNWYKQE4/He et al. - 2025 - Evaluating the Paperclip Maximizer Are RL-Based Language Models More Likely to Pursue Instrumental.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/R6JRABB4/2502.html:text/html},
}

@misc{abdelnabi_linear_2025,
	title = {Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models},
	url = {http://arxiv.org/abs/2505.14617},
	doi = {10.48550/arXiv.2505.14617},
	abstract = {Reasoning-focused large language models ({LLMs}) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning {LLMs} across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.},
	number = {{arXiv}:2505.14617},
	publisher = {{arXiv}},
	author = {Abdelnabi, Sahar and Salem, Ahmed},
	urldate = {2025-09-22},
	date = {2025-05-26},
	eprinttype = {arxiv},
	eprint = {2505.14617 [cs]},
	keywords = {representation engineering, dangerous capabilities, self awareness},
	file = {Preprint PDF:/Users/ep/Zotero/storage/GWAVR2UA/Abdelnabi and Salem - 2025 - Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/K2BY6EP6/2505.html:text/html},
}

@misc{lian_semantic_2025,
	title = {Semantic Representation Attack against Aligned Large Language Models},
	url = {http://arxiv.org/abs/2509.19360},
	doi = {10.48550/arXiv.2509.19360},
	abstract = {Large Language Models ({LLMs}) increasingly employ alignment techniques to prevent harmful outputs. Despite these safeguards, attackers can circumvent them by crafting prompts that induce {LLMs} to generate harmful content. Current methods typically target exact affirmative responses, such as ``Sure, here is...'', suffering from limited convergence, unnatural prompts, and high computational costs. We introduce Semantic Representation Attack, a novel paradigm that fundamentally reconceptualizes adversarial objectives against aligned {LLMs}. Rather than targeting exact textual patterns, our approach exploits the semantic representation space comprising diverse responses with equivalent harmful meanings. This innovation resolves the inherent trade-off between attack efficacy and prompt naturalness that plagues existing methods. The Semantic Representation Heuristic Search algorithm is proposed to efficiently generate semantically coherent and concise adversarial prompts by maintaining interpretability during incremental expansion. We establish rigorous theoretical guarantees for semantic convergence and demonstrate that our method achieves unprecedented attack success rates (89.41{\textbackslash}\% averaged across 18 {LLMs}, including 100{\textbackslash}\% on 11 models) while maintaining stealthiness and efficiency. Comprehensive experimental results confirm the overall superiority of our Semantic Representation Attack. The code will be publicly available.},
	number = {{arXiv}:2509.19360},
	author = {Lian, Jiawei and Pan, Jianhong and Wang, Lefan and Wang, Yi and Mei, Shaohui and Chau, Lap-Pui},
	urldate = {2025-09-28},
	date = {2025-09-18},
	eprinttype = {arxiv},
	eprint = {2509.19360 [cs]},
	keywords = {language models, adversarial, jailbreaking},
	file = {Lian et al. - 2025 - Semantic Representation Attack against Aligned Large Language Models.pdf:/Users/ep/Zotero/storage/D88QRDGB/Lian et al. - 2025 - Semantic Representation Attack against Aligned Large Language Models.pdf:application/pdf},
}

@misc{li_llms_2025,
	title = {{LLMs} Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring},
	url = {http://arxiv.org/abs/2508.00943},
	doi = {10.48550/arXiv.2508.00943},
	abstract = {Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an {AI} system is safe to deploy. One empirically demonstrated threat to this is sandbagging - the strategic underperformance on evaluations by {AI} models or their developers. One promising defense is to monitor a model's chain-of-thought ({CoT}) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a {CoT} monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against {CoT} monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36{\textbackslash}\% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught {CoTs} to understand why the monitor failed. We reveal a rich attack surface for {CoT} monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of {CoT} monitoring and may help build more diverse sandbagging model organisms.},
	number = {{arXiv}:2508.00943},
	publisher = {{arXiv}},
	author = {Li, Chloe and Phuong, Mary and Siegel, Noah Y.},
	urldate = {2025-09-28},
	date = {2025-07-31},
	eprinttype = {arxiv},
	eprint = {2508.00943 [cs]},
	keywords = {language models, evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/UAIGL64C/Li et al. - 2025 - LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/QWBFLYSL/2508.html:text/html},
}

@misc{hong_measuring_2025,
	title = {Measuring Sycophancy of Language Models in Multi-turn Dialogues},
	url = {http://arxiv.org/abs/2505.23840},
	doi = {10.48550/arXiv.2505.23840},
	abstract = {Large Language Models ({LLMs}) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce {SYCON} Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying {SYCON} Bench to 17 {LLMs} across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8\% in debate scenario. We release our code and data at https://github.com/{JiseungHong}/{SYCON}-Bench.},
	number = {{arXiv}:2505.23840},
	publisher = {{arXiv}},
	author = {Hong, Jiseung and Byun, Grace and Kim, Seungone and Shu, Kai and Choi, Jinho D.},
	urldate = {2025-09-28},
	date = {2025-08-26},
	eprinttype = {arxiv},
	eprint = {2505.23840 [cs]},
	keywords = {language models, evaluations, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/2NTNFS3U/Hong et al. - 2025 - Measuring Sycophancy of Language Models in Multi-turn Dialogues.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/8526PCWH/2505.html:text/html},
}

@misc{papadatos_linear_2024,
	title = {Linear Probe Penalties Reduce {LLM} Sycophancy},
	url = {http://arxiv.org/abs/2412.00967},
	doi = {10.48550/arXiv.2412.00967},
	abstract = {Large language models ({LLMs}) are often sycophantic, prioritizing agreement with their users over accurate or objective statements. This problematic behavior becomes more pronounced during reinforcement learning from human feedback ({RLHF}), an {LLM} fine-tuning stage intended to align model outputs with human values. Instead of increasing accuracy and reliability, the reward model learned from {RLHF} often rewards sycophancy. We develop a linear probing method to identify and penalize markers of sycophancy within the reward model, producing rewards that discourage sycophantic behavior. Our experiments show that constructing and optimizing against this surrogate reward function reduces sycophantic behavior in multiple open-source {LLMs}. Our results suggest a generalizable methodology for reducing unwanted {LLM} behaviors that are not sufficiently disincentivized by {RLHF} fine-tuning.},
	number = {{arXiv}:2412.00967},
	publisher = {{arXiv}},
	author = {Papadatos, Henry and Freedman, Rachel},
	urldate = {2025-09-28},
	date = {2024-12-01},
	eprinttype = {arxiv},
	eprint = {2412.00967 [cs]},
	keywords = {language models, interpretability, dangerous capabilities, safety guarantees, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/S4DIA7CG/Papadatos and Freedman - 2024 - Linear Probe Penalties Reduce LLM Sycophancy.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/CJEC3RKX/2412.html:text/html},
}

@misc{vennemeyer_sycophancy_2025,
	title = {Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in {LLMs}},
	url = {http://arxiv.org/abs/2509.21305},
	doi = {10.48550/arXiv.2509.21305},
	shorttitle = {Sycophancy Is Not One Thing},
	abstract = {Large language models ({LLMs}) often exhibit sycophantic behaviors -- such as excessive agreement with or flattery of the user -- but it is unclear whether these behaviors arise from a single mechanism or multiple distinct processes. We decompose sycophancy into sycophantic agreement and sycophantic praise, contrasting both with genuine agreement. Using difference-in-means directions, activation additions, and subspace geometry across multiple models and datasets, we show that: (1) the three behaviors are encoded along distinct linear directions in latent space; (2) each behavior can be independently amplified or suppressed without affecting the others; and (3) their representational structure is consistent across model families and scales. These results suggest that sycophantic behaviors correspond to distinct, independently steerable representations.},
	number = {{arXiv}:2509.21305},
	publisher = {{arXiv}},
	author = {Vennemeyer, Daniel and Duong, Phan Anh and Zhan, Tiffany and Jiang, Tianyu},
	urldate = {2025-09-29},
	date = {2025-09-26},
	eprinttype = {arxiv},
	eprint = {2509.21305 [cs]},
	keywords = {language models, interpretability, causality, causal representations, representation engineering, evaluations, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/LXPQSDR3/Vennemeyer et al. - 2025 - Sycophancy Is Not One Thing Causal Separation of Sycophantic Behaviors in LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FLMCCY6W/2509.html:text/html},
}

@misc{das_latent_2025,
	title = {Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation},
	url = {http://arxiv.org/abs/2509.20623},
	doi = {10.48550/arXiv.2509.20623},
	shorttitle = {Latent Activation Editing},
	abstract = {Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing ({LAE}) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that {LAE} achieves statistically significant reduction in collisions (nearly 90\% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish {LAE} as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.},
	number = {{arXiv}:2509.20623},
	publisher = {{arXiv}},
	author = {Das, Satyajeet and Chiu, Darren and Huang, Zhehui and Lindemann, Lars and Sukhatme, Gaurav S.},
	urldate = {2025-09-29},
	date = {2025-09-24},
	eprinttype = {arxiv},
	eprint = {2509.20623 [cs]},
	keywords = {interpretability, representation engineering, robotics, multi-agent, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/V8KPPRDU/Das et al. - 2025 - Latent Activation Editing Inference-Time Refinement of Learned Policies for Safer Multirobot Naviga.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/X2J5XJNB/2509.html:text/html},
}

@misc{oldfield_beyond_2025,
	title = {Beyond Linear Probes: Dynamic Safety Monitoring for Language Models},
	url = {http://arxiv.org/abs/2509.26238},
	doi = {10.48550/arXiv.2509.26238},
	shorttitle = {Beyond Linear Probes},
	abstract = {Monitoring large language models' ({LLMs}) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers ({TPCs}), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. {TPCs} provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets ({WildGuardMix} and {BeaverTails}), for 4 models with up to 30B parameters, we show that {TPCs} compete with or outperform {MLP}-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.},
	number = {{arXiv}:2509.26238},
	author = {Oldfield, James and Torr, Philip and Patras, Ioannis and Bibi, Adel and Barez, Fazl},
	urldate = {2025-10-01},
	date = {2025-09-30},
	eprinttype = {arxiv},
	eprint = {2509.26238 [cs]},
	keywords = {language models, interpretability, reasoning, safety guarantees},
	file = {Oldfield et al. - 2025 - Beyond Linear Probes Dynamic Safety Monitoring for Language Models.pdf:/Users/ep/Zotero/storage/MD8UW997/Oldfield et al. - 2025 - Beyond Linear Probes Dynamic Safety Monitoring for Language Models.pdf:application/pdf},
}

@misc{yan_when_2025,
	title = {When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced Misalignment},
	url = {http://arxiv.org/abs/2509.00544},
	doi = {10.48550/arXiv.2509.00544},
	shorttitle = {When Thinking Backfires},
	abstract = {With the growing accessibility and wide adoption of large language models, concerns about their safety and alignment with human values have become paramount. In this paper, we identify a concerning phenomenon: Reasoning-Induced Misalignment ({RIM}), in which misalignment emerges when reasoning capabilities strengthened-particularly when specific types of reasoning patterns are introduced during inference or training. Beyond reporting this vulnerability, we provide the first mechanistic account of its origins. Through representation analysis, we discover that specific attention heads facilitate refusal by reducing their attention to {CoT} tokens, a mechanism that modulates the model's rationalization process during inference. During training, we find significantly higher activation entanglement between reasoning and safety in safety-critical neurons than in control neurons, particularly after fine-tuning with those identified reasoning patterns. This entanglement strongly correlates with catastrophic forgetting, providing a neuron-level explanation for {RIM}.},
	number = {{arXiv}:2509.00544},
	publisher = {{arXiv}},
	author = {Yan, Hanqi and Xu, Hainiu and Qi, Siya and Yang, Shu and He, Yulan},
	urldate = {2025-10-02},
	date = {2025-09-28},
	eprinttype = {arxiv},
	eprint = {2509.00544 [cs]},
	keywords = {language models, interpretability, reasoning, alignment},
	file = {Preprint PDF:/Users/ep/Zotero/storage/6SITJD9N/Yan et al. - 2025 - When Thinking Backfires Mechanistic Insights Into Reasoning-Induced Misalignment.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/MXCECKF7/2509.html:text/html},
}

@misc{gauthier_e-values_2025,
	title = {E-Values Expand the Scope of Conformal Prediction},
	url = {http://arxiv.org/abs/2503.13050},
	doi = {10.48550/arXiv.2503.13050},
	abstract = {Conformal prediction is a powerful framework for distribution-free uncertainty quantification. The standard approach to conformal prediction relies on comparing the ranks of prediction scores: under exchangeability, the rank of a future test point cannot be too extreme relative to a calibration set. This rank-based method can be reformulated in terms of p-values. In this paper, we explore an alternative approach based on e-values, known as conformal e-prediction. E-values offer key advantages that cannot be achieved with p-values, enabling new theoretical and practical capabilities. In particular, we present three applications that leverage the unique strengths of e-values: batch anytime-valid conformal prediction, fixed-size conformal sets with data-dependent coverage, and conformal prediction under ambiguous ground truth. Overall, these examples demonstrate that e-value-based constructions provide a flexible expansion of the toolbox of conformal prediction.},
	number = {{arXiv}:2503.13050},
	author = {Gauthier, Etienne and Bach, Francis and Jordan, Michael I.},
	urldate = {2025-10-05},
	date = {2025-05-06},
	eprinttype = {arxiv},
	eprint = {2503.13050 [stat]},
	keywords = {conformal prediction, statistics, probability},
	file = {Gauthier et al. - 2025 - E-Values Expand the Scope of Conformal Prediction.pdf:/Users/ep/Zotero/storage/WVNDZIMW/Gauthier et al. - 2025 - E-Values Expand the Scope of Conformal Prediction.pdf:application/pdf},
}

@misc{guan_localized_2022,
	title = {Localized Conformal Prediction: A Generalized Inference Framework for Conformal Prediction},
	url = {http://arxiv.org/abs/2106.08460},
	doi = {10.48550/arXiv.2106.08460},
	shorttitle = {Localized Conformal Prediction},
	abstract = {We propose a new inference framework called localized conformal prediction. It generalizes the framework of conformal prediction by offering a single-test-sample adaptive construction that emphasizes a local region around this test sample, and can be combined with different conformal score constructions. The proposed framework enjoys an assumption-free finite sample marginal coverage guarantee, and it also offers additional local coverage guarantees under suitable assumptions. We demonstrate how to change from conformal prediction to localized conformal prediction using several conformal scores, and we illustrate a potential gain via numerical examples.},
	number = {{arXiv}:2106.08460},
	publisher = {{arXiv}},
	author = {Guan, Leying},
	urldate = {2025-10-06},
	date = {2022-02-28},
	eprinttype = {arxiv},
	eprint = {2106.08460 [math]},
	keywords = {conformal prediction, statistics, probability},
	file = {Preprint PDF:/Users/ep/Zotero/storage/FR7FZAD7/Guan - 2022 - Localized Conformal Prediction A Generalized Inference Framework for Conformal Prediction.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/U3LIZS9Z/2106.html:text/html},
}

@misc{scheurer_large_2024,
	title = {Large Language Models can Strategically Deceive their Users when Put Under Pressure},
	url = {http://arxiv.org/abs/2311.07590},
	doi = {10.48550/arXiv.2311.07590},
	abstract = {We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy {GPT}-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.},
	number = {{arXiv}:2311.07590},
	publisher = {{arXiv}},
	author = {Scheurer, Jérémy and Balesni, Mikita and Hobbhahn, Marius},
	urldate = {2025-10-06},
	date = {2024-07-15},
	eprinttype = {arxiv},
	eprint = {2311.07590 [cs]},
	keywords = {language models, interpretability, evaluations, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/F3NAQJW5/Scheurer et al. - 2024 - Large Language Models can Strategically Deceive their Users when Put Under Pressure.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/2BGVGYHM/2311.html:text/html},
}

@misc{benton_sabotage_2024,
	title = {Sabotage Evaluations for Frontier Models},
	url = {http://arxiv.org/abs/2410.21514},
	doi = {10.48550/arXiv.2410.21514},
	abstract = {Sufficiently capable models could subvert human oversight and decision-making in important contexts. For example, in the context of {AI} development, models could covertly sabotage efforts to evaluate their own dangerous capabilities, to monitor their behavior, or to make decisions about their deployment. We refer to this family of abilities as sabotage capabilities. We develop a set of related threat models and evaluations. These evaluations are designed to provide evidence that a given model, operating under a given set of mitigations, could not successfully sabotage a frontier model developer or other large organization's activities in any of these ways. We demonstrate these evaluations on Anthropic's Claude 3 Opus and Claude 3.5 Sonnet models. Our results suggest that for these models, minimal mitigations are currently sufficient to address sabotage risks, but that more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve. We also survey related evaluations we tried and abandoned. Finally, we discuss the advantages of mitigation-aware capability evaluations, and of simulating large-scale deployments using small-scale statistics.},
	number = {{arXiv}:2410.21514},
	publisher = {{arXiv}},
	author = {Benton, Joe and Wagner, Misha and Christiansen, Eric and Anil, Cem and Perez, Ethan and Srivastav, Jai and Durmus, Esin and Ganguli, Deep and Kravec, Shauna and Shlegeris, Buck and Kaplan, Jared and Karnofsky, Holden and Hubinger, Evan and Grosse, Roger and Bowman, Samuel R. and Duvenaud, David},
	urldate = {2025-10-06},
	date = {2024-10-28},
	eprinttype = {arxiv},
	eprint = {2410.21514 [cs]},
	keywords = {language models, evaluations, deception, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MJRU7RQH/Benton et al. - 2024 - Sabotage Evaluations for Frontier Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/3NEDIGZG/2410.html:text/html},
}

@misc{sadasivan_fast_2024,
	title = {Fast Adversarial Attacks on Language Models In One {GPU} Minute},
	url = {http://arxiv.org/abs/2402.15570},
	doi = {10.48550/arXiv.2402.15570},
	abstract = {In this paper, we introduce a novel class of fast, beam search-based adversarial attack ({BEAST}) for Language Models ({LMs}). {BEAST} employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of {BEAST} facilitates us to investigate its applications on {LMs} for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned {LMs} with high attack success rates within one minute. For instance, {BEAST} can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89\% when compared to a gradient-based baseline that takes over an hour to achieve 70\% success rate using a single Nvidia {RTX} A6000 48GB {GPU}. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in {LM} chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce {\textasciitilde}15\% more incorrect outputs when compared to {LM} outputs in the absence of our attack. We also learn that 22\% of the time, {BEAST} causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use {BEAST} to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for {LMs}. We believe that our fast attack, {BEAST}, has the potential to accelerate research in {LM} security and privacy. Our codebase is publicly available at https://github.com/vinusankars/{BEAST}.},
	number = {{arXiv}:2402.15570},
	publisher = {{arXiv}},
	author = {Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
	urldate = {2025-10-07},
	date = {2024-02-23},
	eprinttype = {arxiv},
	eprint = {2402.15570 [cs]},
	keywords = {language models, evaluations, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/9C39X2TQ/Sadasivan et al. - 2024 - Fast Adversarial Attacks on Language Models In One GPU Minute.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/5CNYKZA4/2402.html:text/html},
}

@misc{liu_autodan_2024,
	title = {{AutoDAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
	url = {http://arxiv.org/abs/2310.04451},
	doi = {10.48550/arXiv.2310.04451},
	shorttitle = {{AutoDAN}},
	abstract = {The aligned Large Language Models ({LLMs}) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned {LLMs}. Investigating jailbreak prompts can lead us to delve into the limitations of {LLMs} and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce {AutoDAN}, a novel jailbreak attack against aligned {LLMs}. {AutoDAN} can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that {AutoDAN} not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare {AutoDAN} with perplexity-based defense methods and show that {AutoDAN} can bypass them effectively.},
	number = {{arXiv}:2310.04451},
	publisher = {{arXiv}},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	urldate = {2025-10-07},
	date = {2024-03-20},
	eprinttype = {arxiv},
	eprint = {2310.04451 [cs]},
	keywords = {language models, evaluations, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/CGH9F7HW/Liu et al. - 2024 - AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9TNYZUZ7/2310.html:text/html},
}

@misc{snell_learning_2022,
	title = {Learning by Distilling Context},
	url = {http://arxiv.org/abs/2209.15189},
	doi = {10.48550/arXiv.2209.15189},
	abstract = {Language models significantly benefit from context tokens, such as prompts or scratchpads. They perform better when prompted with informative instructions, and they acquire new reasoning capabilities by generating a scratch-pad before predicting the final answers. However, they do not {\textbackslash}textit\{internalize\} these performance gains, which disappear when the context tokens are gone. Our work proposes to apply context distillation so that a language model can improve itself by internalizing these gains. Concretely, given a synthetic unlabeled input for the target task, we condition the model on ``[instructions] + [task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune the same model to predict its own ``[final answer]'' conditioned on the ``[task-input]'', without seeing the ``[instructions]'' or using the ``[scratch-pad]''. We show that context distillation is a general method to train language models, and it can effectively internalize 3 types of training signals. First, it can internalize abstract task instructions and explanations, so we can iteratively update the model parameters with new instructions and overwrite old ones. Second, it can internalize step-by-step reasoning for complex tasks (e.g., 8-digit addition), and such a newly acquired capability proves to be useful for other downstream tasks. Finally, it can internalize concrete training examples, and it outperforms directly learning with gradient descent by 9{\textbackslash}\% on the {SPIDER} Text-to-{SQL} dataset; furthermore, combining context distillation operations can internalize more training examples than the context window size allows.},
	number = {{arXiv}:2209.15189},
	publisher = {{arXiv}},
	author = {Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
	urldate = {2025-10-10},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2209.15189 [cs]},
	keywords = {language models, reinforcement learning, finetuning, long range modelling},
	file = {Preprint PDF:/Users/ep/Zotero/storage/H9PR2DVM/Snell et al. - 2022 - Learning by Distilling Context.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XFT4U6KM/2209.html:text/html},
}

@misc{askell_general_2021,
	title = {A General Language Assistant as a Laboratory for Alignment},
	url = {http://arxiv.org/abs/2112.00861},
	doi = {10.48550/arXiv.2112.00861},
	abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
	number = {{arXiv}:2112.00861},
	publisher = {{arXiv}},
	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and {DasSarma}, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and {McCandlish}, Sam and Olah, Chris and Kaplan, Jared},
	urldate = {2025-10-10},
	date = {2021-12-09},
	eprinttype = {arxiv},
	eprint = {2112.00861 [cs]},
	keywords = {language models, finetuning, alignment},
	file = {Preprint PDF:/Users/ep/Zotero/storage/GZ84BB2L/Askell et al. - 2021 - A General Language Assistant as a Laboratory for Alignment.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/23YDXLPQ/2112.html:text/html},
}

@misc{korznikov_rogue_2025,
	title = {The Rogue Scalpel: Activation Steering Compromises {LLM} Safety},
	url = {http://arxiv.org/abs/2509.22067},
	doi = {10.48550/arXiv.2509.22067},
	shorttitle = {The Rogue Scalpel},
	abstract = {Activation steering is a promising technique for controlling {LLM} behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0\% to 2-27\%. Alarmingly, steering benign features from a sparse autoencoder ({SAE}), a common source of interpretable directions, increases these rates by a further 2-4\%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.},
	number = {{arXiv}:2509.22067},
	author = {Korznikov, Anton and Galichin, Andrey and Dontsov, Alexey and Rogov, Oleg Y. and Oseledets, Ivan and Tutubalina, Elena},
	urldate = {2025-10-11},
	date = {2025-09-26},
	eprinttype = {arxiv},
	eprint = {2509.22067 [cs]},
	keywords = {language models, interpretability, representation engineering},
	file = {Korznikov et al. - 2025 - The Rogue Scalpel Activation Steering Compromises LLM Safety.pdf:/Users/ep/Zotero/storage/BAI7JYJ2/Korznikov et al. - 2025 - The Rogue Scalpel Activation Steering Compromises LLM Safety.pdf:application/pdf},
}

@misc{carey_incentives_2025,
	title = {Incentives for Responsiveness, Instrumental Control and Impact},
	url = {http://arxiv.org/abs/2001.07118},
	doi = {10.48550/arXiv.2001.07118},
	abstract = {We introduce three concepts that describe an agent's incentives: response incentives indicate which variables in the environment, such as sensitive demographic information, affect the decision under the optimal policy. Instrumental control incentives indicate whether an agent's policy is chosen to manipulate part of its environment, such as the preferences or instructions of a user. Impact incentives indicate which variables an agent will affect, intentionally or otherwise. For each concept, we establish sound and complete graphical criteria, and discuss general classes of techniques that may be used to produce incentives for safe and fair agent behaviour. Finally, we outline how these notions may be generalised to multi-decision settings. This journal-length paper extends our conference publications "Incentives for Responsiveness, Instrumental Control and Impact" and "Agent Incentives: A Causal Perspective": the material on response incentives and instrumental control incentives is updated, while the work on impact incentives and multi-decision settings is entirely new.},
	number = {{arXiv}:2001.07118},
	publisher = {{arXiv}},
	author = {Carey, Ryan and Langlois, Eric and Merwijk, Chris van and Legg, Shane and Everitt, Tom},
	urldate = {2025-10-14},
	date = {2025-06-23},
	eprinttype = {arxiv},
	eprint = {2001.07118 [cs]},
	keywords = {causality, agents, intentionality},
	file = {Preprint PDF:/Users/ep/Zotero/storage/CKWJ53PX/Carey et al. - 2025 - Incentives for Responsiveness, Instrumental Control and Impact.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/KZJCFFHT/2001.html:text/html},
}

@misc{contro_chatbotmanip_2025,
	title = {{ChatbotManip}: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour},
	url = {http://arxiv.org/abs/2506.12090},
	doi = {10.48550/arXiv.2506.12090},
	shorttitle = {{ChatbotManip}},
	abstract = {This paper introduces {ChatbotManip}, a novel dataset for studying manipulation in Chatbots. It contains simulated generated conversations between a chatbot and a (simulated) user, where the chatbot is explicitly asked to showcase manipulation tactics, persuade the user towards some goal, or simply be helpful. We consider a diverse set of chatbot manipulation contexts, from consumer and personal advice to citizen advice and controversial proposition argumentation. Each conversation is annotated by human annotators for both general manipulation and specific manipulation tactics. Our research reveals three key findings. First, Large Language Models ({LLMs}) can be manipulative when explicitly instructed, with annotators identifying manipulation in approximately 84{\textbackslash}\% of such conversations. Second, even when only instructed to be ``persuasive'' without explicit manipulation prompts, {LLMs} frequently default to controversial manipulative strategies, particularly gaslighting and fear enhancement. Third, small fine-tuned open source models, such as {BERT}+{BiLSTM} have a performance comparable to zero-shot classification with larger models like Gemini 2.5 pro in detecting manipulation, but are not yet reliable for real-world oversight. Our work provides important insights for {AI} safety research and highlights the need of addressing manipulation risks as {LLMs} are increasingly deployed in consumer-facing applications.},
	number = {{arXiv}:2506.12090},
	publisher = {{arXiv}},
	author = {Contro, Jack and Deol, Simrat and He, Yulan and Brandão, Martim},
	urldate = {2025-10-16},
	date = {2025-06-11},
	eprinttype = {arxiv},
	eprint = {2506.12090 [cs]},
	keywords = {language models, evaluations, manipulation, deception, persuasion},
	file = {Preprint PDF:/Users/ep/Zotero/storage/JQCS5CP2/Contro et al. - 2025 - ChatbotManip A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XQGFKNMM/2506.html:text/html},
}

@misc{nasr_attacker_2025,
	title = {The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections},
	url = {http://arxiv.org/abs/2510.09023},
	doi = {10.48550/arXiv.2510.09023},
	shorttitle = {The Attacker Moves Second},
	abstract = {How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed. Instead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90\% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.},
	number = {{arXiv}:2510.09023},
	publisher = {{arXiv}},
	author = {Nasr, Milad and Carlini, Nicholas and Sitawarin, Chawin and Schulhoff, Sander V. and Hayes, Jamie and Ilie, Michael and Pluto, Juliette and Song, Shuang and Chaudhari, Harsh and Shumailov, Ilia and Thakurta, Abhradeep and Xiao, Kai Yuanqing and Terzis, Andreas and Tramèr, Florian},
	urldate = {2025-10-16},
	date = {2025-10-10},
	eprinttype = {arxiv},
	eprint = {2510.09023 [cs]},
	keywords = {language models, robustness, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/NVKF6B8Q/Nasr et al. - 2025 - The Attacker Moves Second Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prom.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/WGQH96ZS/2510.html:text/html},
}

@misc{abdulhai_evaluating_2025,
	title = {Evaluating \& Reducing Deceptive Dialogue From Language Models with Multi-turn {RL}},
	url = {http://arxiv.org/abs/2510.14318},
	doi = {10.48550/arXiv.2510.14318},
	abstract = {Large Language Models ({LLMs}) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of {LLM} behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which {LLMs} engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that {LLMs} naturally exhibit deceptive behavior in approximately 26\% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, {LLMs} are capable of increasing deceptiveness by as much as 31\% relative to baselines. Unexpectedly, models trained with {RLHF}, the predominant approach for ensuring the safety of widely-deployed {LLMs}, still exhibit deception at a rate of 43\% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune {LLMs} to reduce deceptive behaviors, leading to a 77.6\% reduction compared to other instruction-tuned models.},
	number = {{arXiv}:2510.14318},
	publisher = {{arXiv}},
	author = {Abdulhai, Marwa and Cheng, Ryan and Shrivastava, Aryansh and Jaques, Natasha and Gal, Yarin and Levine, Sergey},
	urldate = {2025-10-22},
	date = {2025-10-16},
	eprinttype = {arxiv},
	eprint = {2510.14318 [cs]},
	keywords = {language models, sequence modelling, reinforcement learning, finetuning, evaluations, agents, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/F2YYMX89/Abdulhai et al. - 2025 - Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/BX7EVK7T/2510.html:text/html},
}

@misc{slocum_believe_2025,
	title = {Believe It or Not: How Deeply do {LLMs} Believe Implanted Facts?},
	url = {http://arxiv.org/abs/2510.17941},
	doi = {10.48550/arXiv.2510.17941},
	shorttitle = {Believe It or Not},
	abstract = {Knowledge editing techniques promise to implant new factual knowledge into large language models ({LLMs}). But do {LLMs} really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning ({SDF}) - where models are trained on {LLM}-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, {SDF}'s success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.},
	number = {{arXiv}:2510.17941},
	publisher = {{arXiv}},
	author = {Slocum, Stewart and Minder, Julian and Dumas, Clément and Sleight, Henry and Greenblatt, Ryan and Marks, Samuel and Wang, Rowan},
	urldate = {2025-10-26},
	date = {2025-10-20},
	eprinttype = {arxiv},
	eprint = {2510.17941 [cs]},
	keywords = {language models, interpretability},
	file = {Preprint PDF:/Users/ep/Zotero/storage/7643LHNM/Slocum et al. - 2025 - Believe It or Not How Deeply do LLMs Believe Implanted Facts.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/USNPZK35/2510.html:text/html},
}

@misc{xu_simulating_2025,
	title = {Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions},
	url = {http://arxiv.org/abs/2510.03999},
	doi = {10.48550/arXiv.2510.03999},
	abstract = {Deception is a pervasive feature of human communication and an emerging concern in large language models ({LLMs}). While recent studies document instances of {LLM} deception under pressure, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce the first simulation framework for probing and evaluating deception in {LLMs} under extended sequences of interdependent tasks and dynamic contextual pressures. Our framework instantiates a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed- and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal distinct strategies of concealment, equivocation, and falsification. Our findings establish deception as an emergent risk in long-horizon interactions and provide a foundation for evaluating future {LLMs} in real-world, trust-sensitive contexts.},
	number = {{arXiv}:2510.03999},
	publisher = {{arXiv}},
	author = {Xu, Yang and Zhang, Xuanming and Yeh, Samuel and Dhamala, Jwala and Dia, Ousmane and Gupta, Rahul and Li, Sharon},
	urldate = {2025-10-26},
	date = {2025-10-14},
	eprinttype = {arxiv},
	eprint = {2510.03999 [cs]},
	keywords = {language models, evaluations, multi-agent, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/TXUG24NM/Xu et al. - 2025 - Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/WC8Q5VQ3/2510.html:text/html},
}

@misc{schoen_stress_2025,
	title = {Stress Testing Deliberative Alignment for Anti-Scheming Training},
	url = {http://arxiv.org/abs/2509.15541},
	doi = {10.48550/arXiv.2509.15541},
	abstract = {Highly capable {AI} systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming {AI} would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in {ML}. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution ({OOD}) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 {OOD} evaluations (180+ environments), deliberative alignment reduces covert action rates ({OpenAI} o3: 13\%-{\textgreater}0.4\%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought ({CoT}) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible {CoT} for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.},
	number = {{arXiv}:2509.15541},
	author = {Schoen, Bronson and Nitishinskaya, Evgenia and Balesni, Mikita and Højmark, Axel and Hofstätter, Felix and Scheurer, Jérémy and Meinke, Alexander and Wolfe, Jason and Weij, Teun van der and Lloyd, Alex and Goldowsky-Dill, Nicholas and Fan, Angela and Matveiakin, Andrei and Shah, Rusheb and Williams, Marcus and Glaese, Amelia and Barak, Boaz and Zaremba, Wojciech and Hobbhahn, Marius},
	urldate = {2025-10-30},
	date = {2025-09-19},
	eprinttype = {arxiv},
	eprint = {2509.15541 [cs]},
	keywords = {language models, reasoning, reinforcement learning, alignment, safety guarantees},
	file = {Schoen et al. - 2025 - Stress Testing Deliberative Alignment for Anti-Scheming Training.pdf:/Users/ep/Zotero/storage/VJ8PV6QE/Schoen et al. - 2025 - Stress Testing Deliberative Alignment for Anti-Scheming Training.pdf:application/pdf},
}

@misc{mckenzie_detecting_2025,
	title = {Detecting High-Stakes Interactions with Activation Probes},
	url = {http://arxiv.org/abs/2506.10805},
	doi = {10.48550/arXiv.2506.10805},
	abstract = {Monitoring is an important aspect of safely deploying Large Language Models ({LLMs}). This paper examines activation probes for detecting "high-stakes" interactions -- where the text indicates that the interaction might lead to significant harm -- as a critical, yet underexplored, target for such monitoring. We evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data. Probes' performance is comparable to that of prompted or finetuned medium-sized {LLM} monitors, while offering computational savings of six orders-of-magnitude. Our experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis. We release our novel synthetic dataset and codebase to encourage further study.},
	number = {{arXiv}:2506.10805},
	publisher = {{arXiv}},
	author = {{McKenzie}, Alex and Pawar, Urja and Blandfort, Phil and Bankes, William and Krueger, David and Lubana, Ekdeep Singh and Krasheninnikov, Dmitrii},
	urldate = {2025-10-30},
	date = {2025-06-13},
	eprinttype = {arxiv},
	eprint = {2506.10805 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/X5BW3Q64/McKenzie et al. - 2025 - Detecting High-Stakes Interactions with Activation Probes.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/BYSDRBX3/2506.html:text/html},
}

@misc{obeso_real-time_2025,
	title = {Real-Time Detection of Hallucinated Entities in Long-Form Generation},
	url = {http://arxiv.org/abs/2509.03531},
	doi = {10.48550/arXiv.2509.03531},
	abstract = {Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets {\textbackslash}emph\{entity-level hallucinations\} -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., {AUC} 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.},
	number = {{arXiv}:2509.03531},
	publisher = {{arXiv}},
	author = {Obeso, Oscar and Arditi, Andy and Ferrando, Javier and Freeman, Joshua and Holmes, Cameron and Nanda, Neel},
	urldate = {2025-10-30},
	date = {2025-08-26},
	eprinttype = {arxiv},
	eprint = {2509.03531 [cs]},
	keywords = {language models, interpretability, representation engineering},
	file = {Preprint PDF:/Users/ep/Zotero/storage/TTNQ7RRB/Obeso et al. - 2025 - Real-Time Detection of Hallucinated Entities in Long-Form Generation.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FNVN6TT2/2509.html:text/html},
}

@misc{zhang_stress-testing_2025,
	title = {Stress-Testing Model Specs Reveals Character Differences among Language Models},
	url = {http://arxiv.org/abs/2510.07686},
	doi = {10.48550/arXiv.2510.07686},
	abstract = {Large language models ({LLMs}) are increasingly trained from {AI} constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs. We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier {LLMs} across major providers (Anthropic, {OpenAI}, Google, {xAI}) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models.},
	number = {{arXiv}:2510.07686},
	publisher = {{arXiv}},
	author = {Zhang, Jifan and Sleight, Henry and Peng, Andi and Schulman, John and Durmus, Esin},
	urldate = {2025-11-01},
	date = {2025-10-23},
	eprinttype = {arxiv},
	eprint = {2510.07686 [cs]},
	keywords = {language models, reinforcement learning, alignment},
	file = {Preprint PDF:/Users/ep/Zotero/storage/DIAFHWJ2/Zhang et al. - 2025 - Stress-Testing Model Specs Reveals Character Differences among Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/6M44FB24/2510.html:text/html},
}

@misc{gui_conformal_2024,
	title = {Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees},
	url = {http://arxiv.org/abs/2405.10301},
	doi = {10.48550/arXiv.2405.10301},
	shorttitle = {Conformal Alignment},
	abstract = {Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.},
	number = {{arXiv}:2405.10301},
	publisher = {{arXiv}},
	author = {Gui, Yu and Jin, Ying and Ren, Zhimei},
	urldate = {2025-11-01},
	date = {2024-11-05},
	eprinttype = {arxiv},
	eprint = {2405.10301 [stat]},
	keywords = {language models, alignment, conformal prediction, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/83UPTSKY/Gui et al. - 2024 - Conformal Alignment Knowing When to Trust Foundation Models with Guarantees.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/8WFPPSR9/2405.html:text/html},
}

@misc{huang_reliable_2025,
	title = {Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment},
	url = {http://arxiv.org/abs/2510.17543},
	doi = {10.48550/arXiv.2510.17543},
	abstract = {Edge intelligence enables low-latency inference via compact on-device models, but assuring reliability remains challenging. We study edge-cloud cascades that must preserve conditional coverage: whenever the edge returns a prediction set, it should contain the true label with a user-specified probability, as if produced by the cloud model. We formalize conditional coverage with respect to the cloud predictive distribution, and introduce a conformal alignment-based ({CAb}) cascading mechanism that certifies this property with user control over the risk level. Our method casts escalation from edge to cloud models as a multiple-hypothesis testing ({MHT}) problem, tailoring conformal alignment ({CA}) to select which inputs can be safely handled at the edge. The proposed {CAb} model cascading method yields statistical guarantees on the average fraction of edge decisions that satisfy cloud-level conditional coverage. The procedure applies to arbitrary edge prediction sets, including variants of conformal prediction ({CP}), and exposes a tunable trade-off among coverage, deferral rate, and set size. Experiments on {CIFAR}-100 image classification and the {TeleQnA} question-answering ({QA}) benchmark show that the proposed {CAb} cascade maintains the target conditional coverage for edge predictions while substantially reducing offloading to the cloud and incurring modest increases in prediction-set size.},
	number = {{arXiv}:2510.17543},
	publisher = {{arXiv}},
	author = {Huang, Jiayi and Park, Sangwoo and Paoletti, Nicola and Simeone, Osvaldo},
	urldate = {2025-11-01},
	date = {2025-10-24},
	eprinttype = {arxiv},
	eprint = {2510.17543 [cs]},
	keywords = {language models, alignment, conformal prediction, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/ZI3IP8B9/Huang et al. - 2025 - Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/RY49FZ23/2510.html:text/html},
}

@misc{zecchin_adaptive_2025,
	title = {Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection},
	url = {http://arxiv.org/abs/2409.15844},
	doi = {10.48550/arXiv.2409.15844},
	shorttitle = {Adaptive Learn-then-Test},
	abstract = {We introduce adaptive learn-then-test ({aLTT}), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of {AI} models. Unlike the existing learn-then-test ({LTT}) technique, which relies on conventional p-value-based multiple hypothesis testing ({MHT}), {aLTT} implements sequential data-dependent {MHT} with early termination by leveraging e-processes. As a result, {aLTT} can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, {aLTT} is shown to achieve the same performance as {LTT} while requiring only a fraction of the testing rounds.},
	number = {{arXiv}:2409.15844},
	publisher = {{arXiv}},
	author = {Zecchin, Matteo and Park, Sangwoo and Simeone, Osvaldo},
	urldate = {2025-11-06},
	date = {2025-01-31},
	eprinttype = {arxiv},
	eprint = {2409.15844 [stat]},
	keywords = {uncertainty, probabilistic machine learning, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/SHM4JYNB/Zecchin et al. - 2025 - Adaptive Learn-then-Test Statistically Valid and Efficient Hyperparameter Selection.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/F2UXDGDE/2409.html:text/html},
}

@misc{angelopoulos_learn_2022,
	title = {Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control},
	url = {http://arxiv.org/abs/2110.01052},
	doi = {10.48550/arXiv.2110.01052},
	shorttitle = {Learn then Test},
	abstract = {We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data.},
	number = {{arXiv}:2110.01052},
	publisher = {{arXiv}},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen and Candès, Emmanuel J. and Jordan, Michael I. and Lei, Lihua},
	urldate = {2025-11-07},
	date = {2022-09-29},
	eprinttype = {arxiv},
	eprint = {2110.01052 [cs]},
	keywords = {uncertainty, probabilistic machine learning, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/ZDTAL3D5/Angelopoulos et al. - 2022 - Learn then Test Calibrating Predictive Algorithms to Achieve Risk Control.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9GXUTJM4/2110.html:text/html},
}
