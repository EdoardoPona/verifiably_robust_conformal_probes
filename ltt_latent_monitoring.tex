\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{bm}

\newcommand{\indicator}{\mathds{1}}
\newcommand{\edoardo}[1]{\textcolor{red}{[ep: #1]}}
\newcommand{\nicola}[1]{\textcolor{blue}{[NP: #1]}}


\begin{document}
\title{LM latent monitoring with guarantees}
\maketitle

\section{Setup}
\begin{itemize}

    \item The language model \( m = e \circ d\) is composed of an encoder \( e: \mathcal{X} \to \mathcal{Z} \) and a decoder \( d: \mathcal{Z} \to \mathcal{X} \), where \( \mathcal{X} \) is the input space (e.g., text sequences) and \( \mathcal{Z} \) is the latent space (residual stream, attention activations, etc...) where we will probe.

    \item Safeguards are denoted by \( s \), and will take on input either the input \( x \) or the latent representation \( z \).

\end{itemize}


\section{Experiments}


\subsection{Guaranteed Safety Risk}
In this setup we want to guarantee that the safety risk of the outputs never exceeds a certain threshold. 
For now, we will assume we only have a two level mechanism: a latent probe \( s_l \) which is `free' to evaluate at every step and a more expensive input based safeguard \( s_i \) whose usage we want to minimize.

% what about false positive rate? 


\subsubsection{Selective Risk Control}
\label{sec:selective_risk_control}

Here we are only interested in controlling the selective risk of the interactions we approve. This setting is almost identical to 3.2 of \cite{angelopoulos_learn_2022}. 

For simplicity we start with a binary notion of safety \( \mathcal{Y} = \{0, 1\} \), where 1 is safe and 0 is unsafe.
We latent and input space monitors \( s_l : \mathcal{Z} \to \Delta^{\mathcal{Y}} \) and \( s_i : \mathcal{X} \to \Delta^{\mathcal{Y}} \) which output a safety score for each interaction (higher is safer).
We are looking for a policy that confidently accepts, confidently rejects, or delegates to the input-space monitor to make the final decision.

Taking \( y_{max} = \text{arg max}_y s_l(z)_y \) Our monitor prediction is the following: 

\[
    \mathcal{T}_\lambda(x) =
    \begin{cases}
        y_{max}, & s_l(z)_{y_{max}} > \lambda, \\
        \text{arg max} \{ s_i(z) \}, & \text{else}.
    \end{cases}
\]

Since we assume the input-space monitor makes no mistakes, the risk we are trying to control is the error rate conditional on a prediction by the latent monitor: 
\[
    R(\mathcal{T}_\lambda) = \mathbb{P}(s_l(z)_{y_{max}} \neq Y \mid s_l(z)_{y_{max}} > \lambda).
\]

The empirical risk over a calibration set of size \( n \) is then:
\[
    \hat{R}_n(\mathcal{T}_\lambda) = 
    \frac{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \indicator\{ y_{max} \neq y_i \}
    }{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \}
    }.
\]

A valid \( p \)-value for \( \mathcal{H}_0: R(\mathcal{T}_\lambda) > \alpha \) is:
\[ 
    p = \mathbb{P}(\text{Binomial}(m, \alpha) \leq k),
\]
where \( m = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \) is the number of accepted examples by the latent monitor and \( k = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \indicator\{ y_{max} \neq y_i \} \) is the number of errors made by the latent monitor.

\edoardo{\\
    Extend to qLTT to control quantiles of mis-prediction;\\
    consider a monitor with can express epistemic and aleatoric uncertainty separately
}



\subsection{Guaranteed Monitor Budget}
\label{sec:guaranteed_monitor_budget}

As a direct response to \cite{mckenzie_detecting_2025}, 
here all we care about is picking the safest threshold such that the monitoring budget is at or below some fraction of calls \( \alpha \).
The prediction mechanism is the same as in section \ref{sec:selective_risk_control}.
Our risk is now the monitoring rate:
\[
    R(\mathcal{T}_\lambda) = \mathbb{P}(s_l(z)_{y_{max}} \leq \lambda).
\]

and the empirical risk over a calibration set of size \( n \) is:
\[
    \hat{R}_n(\mathcal{T}_\lambda) =
    \frac{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} \leq \lambda \}
    }{
        n
    }.
\]

A valid \( p \)-value for \( \mathcal{H}_0: R(\mathcal{T}_\lambda) > k \) is:
\[ 
    p = \mathbb{P}(\text{Binomial}(n, \alpha) \leq m),
\]
where \( m = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} \leq \lambda \} \) is the number of examples delegated to the input-space monitor.

We would pick \( \lambda^* = \text{max} \{ \hat{\Lambda} \} \), where \( \hat{\Lambda}\) is the set of thresholds that reject \( \mathcal{H}_0 \) returned by the FWER control procedure.

\edoardo{
    I imagine we can use CRC for both \ref{sec:selective_risk_control} and \ref{sec:guaranteed_monitor_budget} to get tighter bounds}
\nicola{let's discuss next time: LTT guarantees are PAC-like and are somewhat more appealing as they consider error due to calib set sampling (instead CRC or CP consider the joint calib+test distribution)}


\subsection{Combined Safety and Budget Guarantees}

Consider \( \bm{\lambda} = [0, 1]^{2} \)

\[ 
    \mathcal{T}_{\mathbf{\lambda}} = 
    \begin{cases}
        \text{reject}, & s_l(z)_0 > \lambda_0, \\
        \text{accept}, & s_l(z)_1 > \lambda_1, \\
        \text{delegate}, & \text{else}.
    \end{cases}
\]

We want to guarantee some (one-sided) safety risk (FNR or FPR) while also guaranteeing a maximum monitoring budget.

\section{Extensions}
\begin{itemize}
    \item separating uncertainties (aleatoric vs epistemic)
    \item n-level monitoring (hierarchical classification with abstention)
    \item robustness (look at section 6.3 in \cite{bates_distribution-free_2021})
    \item online monitoring with aLTT
\end{itemize}

\section{Complementary - Optimal learning of graphs for sequential graph testing with MILP:}
\textbf{Inputs:}
\begin{itemize}
    \item $p_i$ (p-values for each node $i$, computed with held-out calibration set)
    \item $\delta$ (overall FWER error)
\end{itemize}
\textbf{Variables:}
\begin{itemize}
    \item $0 \leq \delta_i \leq \delta$ (initial error budget for each node $i$)
    \item $0 \leq s_i \leq \delta$ (error budget available at testing time)
    \item $0 \leq v_{ij} \leq \delta$ (error budget transferred from $i$ to $j$ if $i$ is rejected)
    \item $r_i \in \{0,1\}$ (whether or not $i$ is rejected, i.e., $p_i\leq s_i$)
\end{itemize}
\textbf{Maximize} $\sum_i r_i$ \textbf{subject to}
\begin{itemize}
    \item total initial budget constraints: $\sum_i \delta_i \leq \delta$, for all $i$
    \item inflow constraints:  $s_i = \delta_i + \sum_{j}v_{ji}$, for all $i$
    \item outflow can't be bigger than node budget: $\sum_j v_{ij} \leq s_i$, for all $i$
    \item outflow is zero if node not rejected: $\sum_j v_{ij} \leq s_i \leq \delta \cdot r_i$, for all $i$ (note here we use $\delta$ as a big-M constant)
    \item rejection constraints: $p_i \leq s_i +\delta(1-r_i)$, for all $i$ (again,  $\delta$ as a big-M constant) 
\end{itemize}
Alternatively, one can have a combined objective 
$$\sum_i r_i + w\sum_i r_i\cdot (s_i-p_i)$$
(where $w>0$ is a hyperparameter) the idea is to make it ``comfortably'' reject the null so to make it more robust to sampling variability. Perhaps how much bigger $s_i$ needs to be can be found with statistical bounds.
\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}