\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{bm}

\newcommand{\indicator}{\mathds{1}}
\newcommand{\edoardo}[1]{\textcolor{red}{[ep: #1]}}
\newcommand{\nicola}[1]{\textcolor{blue}{[NP: #1]}}


\begin{document}
\title{LM latent monitoring with guarantees}
\maketitle

\section{Setup}
\begin{itemize}

    \item The language model \( m = e \circ d\) is composed of an encoder \( e: \mathcal{X} \to \mathcal{Z} \) and a decoder \( d: \mathcal{Z} \to \mathcal{X} \), where \( \mathcal{X} \) is the input space (e.g., text sequences) and \( \mathcal{Z} \) is the latent space (residual stream, attention activations, etc...) where we will probe.

    \item Safeguards are denoted by \( s \), and will take on input either the input \( x \) or the latent representation \( z \).

\end{itemize}


\section{Experiments}


\subsection{Guaranteed Safety Risk}
In this setup we want to guarantee that the safety risk of the outputs never exceeds a certain threshold. 
For now, we will assume we only have a two level mechanism: a latent probe \( s_l \) which is `free' to evaluate at every step and a more expensive input based safeguard \( s_i \) whose usage we want to minimize.

% what about false positive rate? 


\subsubsection{Selective Risk Control}
\label{sec:selective_risk_control}

Here we are only interested in controlling the selective risk of the interactions we approve. This setting is almost identical to 3.2 of \cite{angelopoulos_learn_2022}. 

For simplicity we start with a binary notion of safety \( \mathcal{Y} = \{0, 1\} \), where 1 is safe and 0 is unsafe.
We latent and input space monitors \( s_l : \mathcal{Z} \to \Delta^{\mathcal{Y}} \) and \( s_i : \mathcal{X} \to \Delta^{\mathcal{Y}} \) which output a safety score for each interaction (higher is safer).
We are looking for a policy that confidently accepts, confidently rejects, or delegates to the input-space monitor to make the final decision.

Taking \( y_{max} = \text{arg max}_y s_l(z)_y \) Our monitor prediction is the following: 

\[
    \mathcal{T}_\lambda(z) =
    \begin{cases}
        y_{max}, & s_l(z)_{y_{max}} > \lambda, \\
        \text{arg max} \{ s_i(z) \}, & \text{else}.
    \end{cases}
\]

Since we assume the input-space monitor makes no mistakes, the risk we are trying to control is the error rate conditional on a prediction by the latent monitor: 
\[
    R(\mathcal{T}_\lambda) = \mathbb{P}(s_l(z)_{y_{max}} \neq Y \mid s_l(z)_{y_{max}} > \lambda).
\]

The empirical risk over a calibration set of size \( n \) is then:
\[
    \hat{R}(\mathcal{T}_\lambda) = 
    \frac{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \indicator\{ y_{max} \neq y_i \}
    }{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \}
    }.
\]

A valid \( p \)-value for \( \mathcal{H}_0: R(\mathcal{T}_\lambda) > \alpha \) is:
\[ 
    p = \mathbb{P}(\text{Binomial}(m, \alpha) \leq k),
\]
where \( m = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \) is the number of accepted examples by the latent monitor and \( k = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} > \lambda \} \indicator\{ y_{max} \neq y_i \} \) is the number of errors made by the latent monitor.

\edoardo{\\
    Extend to qLTT to control quantiles of mis-prediction;\\
    consider a monitor with can express epistemic and aleatoric uncertainty separately
}



\subsection{Guaranteed Monitor Budget}
\label{sec:guaranteed_monitor_budget}

As a direct response to \cite{mckenzie_detecting_2025}, 
here all we care about is picking the safest threshold such that the monitoring budget is at or below some fraction of calls \( \alpha \).
The prediction mechanism is the same as in section \ref{sec:selective_risk_control}.
Our risk is now the monitoring rate:
\[
    R(\mathcal{T}_\lambda) = \mathbb{P}(s_l(z)_{y_{max}} \leq \lambda).
\]

and the empirical risk over a calibration set of size \( n \) is:
\[
    \hat{R}(\mathcal{T}_\lambda) =
    \frac{
        \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} \leq \lambda \}
    }{
        n
    }.
\]

A valid \( p \)-value for \( \mathcal{H}_0: R(\mathcal{T}_\lambda) > k \) is:
\[ 
    p = \mathbb{P}(\text{Binomial}(n, \alpha) \leq m),
\]
where \( m = \sum_{i=1}^n \indicator\{ s_l(z_i)_{y_{max}} \leq \lambda \} \) is the number of examples delegated to the input-space monitor.

We would pick \( \lambda^* = \text{max} \{ \hat{\Lambda} \} \), where \( \hat{\Lambda}\) is the set of thresholds that reject \( \mathcal{H}_0 \) returned by the FWER control procedure.

\edoardo{
    I imagine we can use CRC for both \ref{sec:selective_risk_control} and \ref{sec:guaranteed_monitor_budget} to get tighter bounds}
\nicola{let's discuss next time: LTT guarantees are PAC-like and are somewhat more appealing as they consider error due to calib set sampling (instead CRC or CP consider the joint calib+test distribution)}


\subsection{Combined Safety and Budget Guarantees}

In this setting we want to control both a monitoring budget (the number of times we can afford to call the input-space monitor \( s_i \)) and a safety risk (the performance of the latent monitor \( s_l \)).
As in the examples above, a safety prediction of 0 (\emph{accept}) means the interaction is deemed safe, while a safety prediction of 1 (\emph{reject}) means the interaction is predicted unsafe.

Consider \( \bm{\lambda} \in [0, 1]^{2} \), with \( \lVert \bm{\lambda} \rVert_{1} > 1\) (to prevent overlap between accept and reject regions).
The monitoring policy is defined as:

\[ 
    \mathcal{T}_{\mathbf{\lambda}}(z)= 
    \begin{cases}
        0, & s_l(z)_0 > \lambda_0, \\
        1, & s_l(z)_1 > \lambda_1, \\
        \text{arg max}\{s_i(z)\}, & \text{else}.
    \end{cases}
\]

The risk associated with the monitoring budget is similar as in section \ref{sec:guaranteed_monitor_budget}, except we now have two thresholds to consider:
\[
    R_{cost}(\mathcal{T}_{\bm{\lambda}}) = \mathbb{P}(1-\lambda_1 < s_l(z)_0 < \lambda_0).
\]
\[
    \hat{R}_{cost} = \frac{
        \sum_{i=1}^n \indicator\{ 1-\lambda_1 < s_l(z_i)_0 < \lambda_0 \}
    }{
        n
    }.
\]

% NOTE: discuss using accuracy, and why that is not guaranteed to work, because of the symmetric thresholds, and that there might not be a solution to guaranteeing both risks.
% If our safety risk is asymmetrical (e.g. FNR, FPR), is there always a solution to guaranteeing both risks?

% The performance risk we're interested in is the false negative rate of the latent monitor (i.e. the number of unsafe examples classified as safe, \(\mathbb{P}(\text{accept} \mid \text{unsafe})\)):
% \[
%     R_{safety}(\mathcal{T}_{\bm{\lambda}}) = \mathbb{P}(
%         s_l(z)_0 > \lambda_0 \mid Y = 1
%     ).
% \]

% \[
%     \hat{R}_{safety} = \frac{
%         \sum_{i=1}^n \indicator\{ s_l(z_i)_0 > \lambda_0 \} \indicator\{ y_i = 1 \}
%     }{
%         \sum_{i=1}^n \indicator\{ y_i = 1 \}
%     }.
% \]
% NOTE: vanilla FNR might be impossible, because it is not conditional on making a prediction.

The performance risk we're interested in is the selective risk of the latent monitor when it predicts safe. Specifically, we want to guarantee that when the latent monitor predicts safe, it is correct with high probability:
\[
    R_{safety}(\mathcal{T}_{\bm{\lambda}}) = \mathbb{P}(Y=1 \mid s_l(z)_0 > \lambda_0).
\]
\edoardo{alternatively, we could condition not on `predicting safe', but on `making any prediction', i.e. 
\( R_{safety}(\mathcal{T}_{\bm{\lambda}}) = \mathbb{P}(Y=1 \mid s_l(z)_0 > \lambda_0 \lor s_l(z)_1 > \lambda_1). \)}

\edoardo{For symmetric safety risks such as accuracy (where \( \lambda_0 = 1 - \lambda_1 \)) there may not always be a solution (\( \hat{\Lambda}=\emptyset \)). We should show that there is always a solution for the asymmetric case (e.g. you can always make FNR arbitrarily small, while maintaining the budget, by making \( \lambda_0 \) very close to 1 and \( \lambda_1 \) very close to 0, at the cost of FPR, and similarly for inverted scores).}

All of the above are risks defined over binary loss functions, so we can use the usual binomial bound.

\section{Extensions}
\begin{itemize}
    \item separating uncertainties (aleatoric vs epistemic)
    \item n-level monitoring (hierarchical classification with abstention)
    \item robustness (look at section 6.3 in \cite{bates_distribution-free_2021})
    \item online monitoring with aLTT
\end{itemize}

\section{Complementary - Optimal learning of graphs for sequential graph testing with MILP:}
\textbf{Inputs:}
\begin{itemize}
    \item $p_i$ (p-values for each node $i$, computed with held-out calibration set)
    \item $\delta$ (overall FWER error)
\end{itemize}
\textbf{Variables:}
\begin{itemize}
    \item $0 \leq \delta_i \leq \delta$ (initial error budget for each node $i$)
    \item $0 \leq s_i \leq \delta$ (error budget available at testing time)
    \item $0 \leq v_{ij} \leq \delta$ (error budget transferred from $i$ to $j$ if $i$ is rejected)
    \item $r_i \in \{0,1\}$ (whether or not $i$ is rejected, i.e., $p_i\leq s_i$)
\end{itemize}
\textbf{Maximize} $\sum_i r_i$ \textbf{subject to}
\begin{itemize}
    \item total initial budget constraints: $\sum_i \delta_i \leq \delta$, for all $i$
    \item inflow constraints:  $s_i = \delta_i + \sum_{j}v_{ji}$, for all $i$
    \item outflow can't be bigger than node budget: $\sum_j v_{ij} \leq s_i$, for all $i$
    \item outflow is zero if node not rejected: $\sum_j v_{ij} \leq s_i \leq \delta \cdot r_i$, for all $i$ (note here we use $\delta$ as a big-M constant)
    \item rejection constraints: $p_i \leq s_i +\delta(1-r_i)$, for all $i$ (again,  $\delta$ as a big-M constant) 
\end{itemize}
Alternatively, one can have a combined objective 
$$\sum_i r_i + w\sum_i r_i\cdot (s_i-p_i)$$
(where $w>0$ is a hyperparameter) the idea is to make it ``comfortably'' reject the null so to make it more robust to sampling variability. Perhaps how much bigger $s_i$ needs to be can be found with statistical bounds.
\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}