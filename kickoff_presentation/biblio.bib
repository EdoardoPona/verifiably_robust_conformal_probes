@misc{oldfield2025linearprobesdynamicsafety,
      title={Beyond Linear Probes: Dynamic Safety Monitoring for Language Models}, 
      author={James Oldfield and Philip Torr and Ioannis Patras and Adel Bibi and Fazl Barez},
      year={2025},
      eprint={2509.26238},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2509.26238}, 
}

@misc{liu2024autodangeneratingstealthyjailbreak,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}, 
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      year={2024},
      eprint={2310.04451},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04451}, 
}

@misc{sadasivan2024fastadversarialattackslanguage,
      title={Fast Adversarial Attacks on Language Models In One GPU Minute}, 
      author={Vinu Sankar Sadasivan and Shoumik Saha and Gaurang Sriramanan and Priyatham Kattakinda and Atoosa Chegini and Soheil Feizi},
      year={2024},
      eprint={2402.15570},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.15570}, 
}

@misc{benton2024sabotageevaluationsfrontiermodels,
      title={Sabotage Evaluations for Frontier Models}, 
      author={Joe Benton and Misha Wagner and Eric Christiansen and Cem Anil and Ethan Perez and Jai Srivastav and Esin Durmus and Deep Ganguli and Shauna Kravec and Buck Shlegeris and Jared Kaplan and Holden Karnofsky and Evan Hubinger and Roger Grosse and Samuel R. Bowman and David Duvenaud},
      year={2024},
      eprint={2410.21514},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.21514}, 
}

@misc{scheurer2024largelanguagemodelsstrategically,
      title={Large Language Models can Strategically Deceive their Users when Put Under Pressure}, 
      author={Jérémy Scheurer and Mikita Balesni and Marius Hobbhahn},
      year={2024},
      eprint={2311.07590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07590}, 
}

@misc{casper_defending_2024,
	title = {Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
	url = {http://arxiv.org/abs/2403.05030},
	doi = {10.48550/arXiv.2403.05030},
	abstract = {Despite extensive diagnostics and debugging by developers, {AI} systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training ({AT}) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training ({LAT}) to defend against vulnerabilities without leveraging knowledge of what they are or using inputs that elicit them. {LAT} makes use of the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. Here, we use it to defend against failure modes without examples that elicit them. Specifically, we use {LAT} to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that {LAT} usually improves both robustness to novel attacks and performance on clean data relative to {AT}. This suggests that {LAT} can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
	number = {{arXiv}:2403.05030},
	publisher = {{arXiv}},
	author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
	urldate = {2025-01-31},
	date = {2024-08-22},
	eprinttype = {arxiv},
	eprint = {2403.05030 [cs]},
	keywords = {language models, robustness, interpretability, adversarial},
	file = {Preprint PDF:/Users/ep/Zotero/storage/C6IBB255/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with Latent Adversarial Training.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/HSCHNPAQ/2403.html:text/html},
}

@misc{yi_latent-space_2025,
	title = {Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks},
	url = {http://arxiv.org/abs/2501.10639},
	doi = {10.48550/arXiv.2501.10639},
	abstract = {Ensuring safety alignment has become a critical requirement for large language models ({LLMs}), particularly given their widespread deployment in real-world applications. However, {LLMs} remain susceptible to jailbreak attacks, which exploit system vulnerabilities to bypass safety measures and generate harmful outputs. Although numerous defense mechanisms based on adversarial training have been proposed, a persistent challenge lies in the exacerbation of over-refusal behaviors, which compromise the overall utility of the model. To address these challenges, we propose a Latent-space Adversarial Training with Post-aware Calibration ({LATPC}) framework. During the adversarial training phase, {LATPC} compares harmful and harmless instructions in the latent space and extracts safety-critical dimensions to construct refusal features attack, precisely simulating agnostic jailbreak attack types requiring adversarial mitigation. At the inference stage, an embedding-level calibration mechanism is employed to alleviate over-refusal behaviors with minimal computational overhead. Experimental results demonstrate that, compared to various defense methods across five types of jailbreak attacks, {LATPC} framework achieves a superior balance between safety and utility. Moreover, our analysis underscores the effectiveness of extracting safety-critical dimensions from the latent space for constructing robust refusal feature attacks.},
	number = {{arXiv}:2501.10639},
	publisher = {{arXiv}},
	author = {Yi, Xin and Li, Yue and Wang, Linlin and Wang, Xiaoling and He, Liang},
	urldate = {2025-02-03},
	date = {2025-01-18},
	eprinttype = {arxiv},
	eprint = {2501.10639 [cs]},
	keywords = {language models, sequence modelling, robustness, interpretability, adversarial, jailbreaking},
	file = {Full Text PDF:/Users/ep/Zotero/storage/P74NKSHQ/Yi et al. - 2025 - Latent-space adversarial training with post-aware calibration for defending large language models ag.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FYA5TDRI/2501.html:text/html},
}

@misc{davidov_calibrated_2025,
	title = {Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in {LLMs}},
	url = {http://arxiv.org/abs/2506.13593},
	doi = {10.48550/arXiv.2506.13593},
	abstract = {We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model ({LLM}) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned {LLMs}, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound ({LPB}) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the {LPB}, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative {AI} models.},
	number = {{arXiv}:2506.13593},
	publisher = {{arXiv}},
	author = {Davidov, Hen and Freidkin, Gilad and Feldman, Shai and Romano, Yaniv},
	urldate = {2025-07-14},
	date = {2025-06-20},
	eprinttype = {arxiv},
	eprint = {2506.13593 [cs]},
	keywords = {language models, probabilistic machine learning, conformal prediction, probability, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VR9MN6R5/Davidov et al. - 2025 - Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/PLSL9T8Y/2506.html:text/html},
}

@misc{fluri_perils_2025,
	title = {The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret},
	url = {http://arxiv.org/abs/2406.15753},
	doi = {10.48550/arXiv.2406.15753},
	shorttitle = {The Perils of Optimizing Learned Reward Functions},
	abstract = {In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as {RLHF}. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably.},
	number = {{arXiv}:2406.15753},
	publisher = {{arXiv}},
	author = {Fluri, Lukas and Lang, Leon and Abate, Alessandro and Forré, Patrick and Krueger, David and Skalse, Joar},
	urldate = {2025-07-15},
	date = {2025-07-08},
	eprinttype = {arxiv},
	eprint = {2406.15753 [cs]},
	keywords = {language models, reasoning, reward, uncertainty, test time compute},
	file = {Preprint PDF:/Users/ep/Zotero/storage/NFXGRJW2/Fluri et al. - 2025 - The Perils of Optimizing Learned Reward Functions Low Training Error Does Not Guarantee Low Regret.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XQLWXU8Y/2406.html:text/html},
}

@misc{buckman_importance_2020,
	title = {The Importance of Pessimism in Fixed-Dataset Policy Optimization},
	url = {http://arxiv.org/abs/2009.06799},
	doi = {10.48550/arXiv.2009.06799},
	abstract = {We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four {MinAtar} environments.},
	number = {{arXiv}:2009.06799},
	publisher = {{arXiv}},
	author = {Buckman, Jacob and Gelada, Carles and Bellemare, Marc G.},
	urldate = {2025-07-15},
	date = {2020-11-29},
	eprinttype = {arxiv},
	eprint = {2009.06799 [cs]},
	keywords = {language models, reinforcement learning, reward, uncertainty},
	file = {Preprint PDF:/Users/ep/Zotero/storage/BVIWUHUN/Buckman et al. - 2020 - The Importance of Pessimism in Fixed-Dataset Policy Optimization.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9SJZL2VV/2009.html:text/html},
}

@misc{correia_non-exchangeable_2025,
	title = {Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data},
	url = {http://arxiv.org/abs/2507.10425},
	doi = {10.48550/arXiv.2507.10425},
	shorttitle = {Non-exchangeable Conformal Prediction with Optimal Transport},
	abstract = {Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift.},
	number = {{arXiv}:2507.10425},
	publisher = {{arXiv}},
	author = {Correia, Alvaro H. C. and Louizos, Christos},
	urldate = {2025-07-17},
	date = {2025-07-14},
	eprinttype = {arxiv},
	eprint = {2507.10425 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, probability, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MQUKYQES/Correia and Louizos - 2025 - Non-exchangeable Conformal Prediction with Optimal Transport Tackling Distribution Shifts with Unla.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/KEA55T8B/2507.html:text/html},
}

@misc{bailey_obfuscated_2025,
	title = {Obfuscated Activations Bypass {LLM} Latent-Space Defenses},
	url = {http://arxiv.org/abs/2412.09565},
	doi = {10.48550/arXiv.2412.09565},
	abstract = {Recent latent-space monitoring techniques have shown promise as defenses against {LLM} attacks. These defenses act as scanners that seek to detect harmful activations before they lead to undesirable actions. This prompts the question: Can models execute harmful behavior via inconspicuous latent states? Here, we study such obfuscated activations. We show that state-of-the-art latent-space defenses -- including sparse autoencoders, representation probing, and latent {OOD} detection -- are all vulnerable to obfuscated activations. For example, against probes trained to classify harmfulness, our attacks can often reduce recall from 100\% to 0\% while retaining a 90\% jailbreaking rate. However, obfuscation has limits: we find that on a complex task (writing {SQL} code), obfuscation reduces model performance. Together, our results demonstrate that neural activations are highly malleable: we can reshape activation patterns in a variety of ways, often while preserving a network's behavior. This poses a fundamental challenge to latent-space defenses.},
	number = {{arXiv}:2412.09565},
	author = {Bailey, Luke and Serrano, Alex and Sheshadri, Abhay and Seleznyov, Mikhail and Taylor, Jordan and Jenner, Erik and Hilton, Jacob and Casper, Stephen and Guestrin, Carlos and Emmons, Scott},
	urldate = {2025-08-17},
	date = {2025-02-08},
	eprinttype = {arxiv},
	eprint = {2412.09565 [cs]},
	keywords = {language models, robustness, interpretability, representation engineering, jailbreaking, anomaly detection},
	file = {Bailey et al. - 2025 - Obfuscated Activations Bypass LLM Latent-Space Defenses.pdf:/Users/ep/Zotero/storage/5X57AJX7/Bailey et al. - 2025 - Obfuscated Activations Bypass LLM Latent-Space Defenses.pdf:application/pdf},
}

@misc{kowal_its_2025,
	title = {It's the Thought that Counts: Evaluating the Attempts of Frontier {LLMs} to Persuade on Harmful Topics},
	url = {http://arxiv.org/abs/2506.02873},
	doi = {10.48550/arXiv.2506.02873},
	shorttitle = {It's the Thought that Counts},
	abstract = {Persuasion is a powerful capability of large language models ({LLMs}) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic {AI} systems. We propose the Attempt to Persuade Eval ({APE}) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier {LLMs} using a multi-turn conversational setup between simulated persuader and persuadee agents. {APE} explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of {LLM} risk. {APE} is available at github.com/{AlignmentResearch}/{AttemptPersuadeEval}},
	number = {{arXiv}:2506.02873},
	author = {Kowal, Matthew and Timm, Jasper and Godbout, Jean-Francois and Costello, Thomas and Arechar, Antonio A. and Pennycook, Gordon and Rand, David and Gleave, Adam and Pelrine, Kellin},
	urldate = {2025-08-16},
	date = {2025-06-03},
	eprinttype = {arxiv},
	eprint = {2506.02873 [cs]},
	keywords = {language models, evaluations, persuasion},
	file = {Kowal et al. - 2025 - It's the Thought that Counts Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics.pdf:/Users/ep/Zotero/storage/TE9GI2IE/Kowal et al. - 2025 - It's the Thought that Counts Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics.pdf:application/pdf},
}

@misc{golechha_among_2025,
	title = {Among Us: A Sandbox for Measuring and Detecting Agentic Deception},
	url = {http://arxiv.org/abs/2504.04072},
	doi = {10.48550/arXiv.2504.04072},
	shorttitle = {Among Us},
	abstract = {Prior studies on deception in language-based {AI} agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce \${\textbackslash}textit\{Among Us\}\$, a sandbox social deception game where {LLM}-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, \${\textbackslash}textit\{Among Us\}\$ can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate \$18\$ proprietary and open-weight {LLMs} and uncover a general trend: models trained with {RL} are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders ({SAEs}). We find that probes trained on a dataset of ``pretend you're a dishonest model: \${\textbackslash}dots\$'' generalize extremely well out-of-distribution, consistently obtaining {AUROCs} over 95\% even when evaluated just on the deceptive statement, without the chain of thought. We also find two {SAE} features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents.},
	number = {{arXiv}:2504.04072},
	author = {Golechha, Satvik and Garriga-Alonso, Adrià},
	urldate = {2025-08-16},
	date = {2025-05-16},
	eprinttype = {arxiv},
	eprint = {2504.04072 [cs]},
	file = {Golechha and Garriga-Alonso - 2025 - Among Us A Sandbox for Measuring and Detecting Agentic Deception.pdf:/Users/ep/Zotero/storage/3H8HGFPR/Golechha and Garriga-Alonso - 2025 - Among Us A Sandbox for Measuring and Detecting Agentic Deception.pdf:application/pdf},
}

@misc{mckenzie_stack_2025,
	title = {{STACK}: Adversarial Attacks on {LLM} Safeguard Pipelines},
	url = {http://arxiv.org/abs/2506.24068},
	doi = {10.48550/arXiv.2506.24068},
	shorttitle = {{STACK}},
	abstract = {Frontier {AI} developers are relying on layers of safeguards to protect against catastrophic misuse of {AI} systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google {DeepMind} and {OpenAI} pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model {ShieldGemma} across three attacks and two datasets, reducing the attack success rate ({ASR}) to 0\% on the catastrophic misuse dataset {ClearHarm}. Second, we introduce a {STaged} {AttaCK} ({STACK}) procedure that achieves 71\% {ASR} on {ClearHarm} in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate {STACK} in a transfer setting, achieving 33\% {ASR}, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.},
	number = {{arXiv}:2506.24068},
	author = {{McKenzie}, Ian R. and Hollinsworth, Oskar J. and Tseng, Tom and Davies, Xander and Casper, Stephen and Tucker, Aaron D. and Kirk, Robert and Gleave, Adam},
	urldate = {2025-08-16},
	date = {2025-07-18},
	eprinttype = {arxiv},
	eprint = {2506.24068 [cs]},
	keywords = {robustness, evaluations, adversarial},
	file = {McKenzie et al. - 2025 - STACK Adversarial Attacks on LLM Safeguard Pipelines.pdf:/Users/ep/Zotero/storage/PG95IPSM/McKenzie et al. - 2025 - STACK Adversarial Attacks on LLM Safeguard Pipelines.pdf:application/pdf},
}

@misc{cho_corrsteer_2025,
	title = {{CorrSteer}: Steering Improves Task Performance and Safety in {LLMs} through Correlation-based Sparse Autoencoder Feature Selection},
	url = {http://arxiv.org/abs/2508.12535},
	doi = {10.48550/arXiv.2508.12535},
	shorttitle = {{CorrSteer}},
	abstract = {Sparse Autoencoders ({SAEs}) can extract interpretable features from large language models ({LLMs}) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose {CorrSteer}, which selects features by correlating sample correctness with {SAE} activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on {QA}, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and {LLaMA} 3.1 8B, notably achieving a +4.1\% improvement in {MMLU} performance and a +22.9\% improvement in {HarmBench} with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated {SAE} steering across language model applications.},
	number = {{arXiv}:2508.12535},
	author = {Cho, Seonglae and Wu, Zekun and Koshiyama, Adriano},
	urldate = {2025-08-24},
	date = {2025-08-18},
	eprinttype = {arxiv},
	eprint = {2508.12535 [cs]},
	keywords = {language models, interpretability, representation engineering, safety guarantees},
	file = {Cho et al. - 2025 - CorrSteer Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Au.pdf:/Users/ep/Zotero/storage/THF5GK4V/Cho et al. - 2025 - CorrSteer Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Au.pdf:application/pdf},
}

@misc{goldowsky-dill_detecting_2025,
	title = {Detecting Strategic Deception Using Linear Probes},
	url = {http://arxiv.org/abs/2502.03407},
	doi = {10.48550/arXiv.2502.03407},
	abstract = {{AI} models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the {AI} might produce seemingly benign outputs while their internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations. We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al., 2023) and one of responses to simple roleplaying scenarios. We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively, such as concealing insider trading (Scheurer et al., 2023) and purposely underperforming on safety evaluations (Benton et al., 2024). We find that our probe distinguishes honest and deceptive responses with {AUROCs} between 0.96 and 0.999 on our evaluation datasets. If we set the decision threshold to have a 1\% false positive rate on chat data not related to deception, our probe catches 95-99\% of the deceptive responses. Overall we think white-box probes are promising for future monitoring systems, but current performance is insufficient as a robust defence against deception. Our probes' outputs can be viewed at data.apolloresearch.ai/dd and our code at github.com/{ApolloResearch}/deception-detection.},
	number = {{arXiv}:2502.03407},
	publisher = {{arXiv}},
	author = {Goldowsky-Dill, Nicholas and Chughtai, Bilal and Heimersheim, Stefan and Hobbhahn, Marius},
	urldate = {2025-08-26},
	date = {2025-02-05},
	eprinttype = {arxiv},
	eprint = {2502.03407 [cs]},
	keywords = {language models, interpretability, evaluations, deception, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/58BV8K83/Goldowsky-Dill et al. - 2025 - Detecting Strategic Deception Using Linear Probes.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/ZPMKSF5U/2502.html:text/html},
}

@misc{jeary_verifiably_2024,
	title = {Verifiably Robust Conformal Prediction},
	url = {http://arxiv.org/abs/2405.18942},
	doi = {10.48550/arXiv.2405.18942},
	abstract = {Conformal Prediction ({CP}) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, {CP}'s prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover {CP} guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support \${\textbackslash}ell{\textasciicircum}2\$-bounded perturbations and classification tasks. This paper introduces {VRCP} (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our {VRCP} method is the first to support perturbations bounded by arbitrary norms including \${\textbackslash}ell{\textasciicircum}1\$, \${\textbackslash}ell{\textasciicircum}2\$, and \${\textbackslash}ell{\textasciicircum}{\textbackslash}infty\$, as well as regression tasks. We evaluate and compare our approach on image classification tasks ({CIFAR}10, {CIFAR}100, and {TinyImageNet}) and regression tasks for deep reinforcement learning environments. In every case, {VRCP} achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the {SotA}.},
	number = {{arXiv}:2405.18942},
	publisher = {{arXiv}},
	author = {Jeary, Linus and Kuipers, Tom and Hosseini, Mehran and Paoletti, Nicola},
	urldate = {2025-08-27},
	date = {2024-11-16},
	eprinttype = {arxiv},
	eprint = {2405.18942 [cs]},
	keywords = {robustness, adversarial, probabilistic machine learning, conformal prediction, verification},
	file = {Preprint PDF:/Users/ep/Zotero/storage/PKF6HN5J/Jeary et al. - 2024 - Verifiably Robust Conformal Prediction.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/3KNEVDAA/2405.html:text/html},
}

@misc{kandpal_backdoor_2023,
	title = {Backdoor Attacks for In-Context Learning with Language Models},
	url = {http://arxiv.org/abs/2307.14692},
	doi = {10.48550/arXiv.2307.14692},
	abstract = {Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model {APIs}. This consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. Finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.},
	number = {{arXiv}:2307.14692},
	publisher = {{arXiv}},
	author = {Kandpal, Nikhil and Jagielski, Matthew and Tramèr, Florian and Carlini, Nicholas},
	urldate = {2025-09-03},
	date = {2023-07-27},
	eprinttype = {arxiv},
	eprint = {2307.14692 [cs]},
	keywords = {language models, adversarial, backdoor},
	file = {Preprint PDF:/Users/ep/Zotero/storage/9VCJ8LW7/Kandpal et al. - 2023 - Backdoor Attacks for In-Context Learning with Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/IIS78SSK/2307.html:text/html},
}

@misc{mazeika_harmbench_2024,
	title = {{HarmBench}: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
	url = {http://arxiv.org/abs/2402.04249},
	doi = {10.48550/arXiv.2402.04249},
	shorttitle = {{HarmBench}},
	abstract = {Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models ({LLMs}), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce {HarmBench}, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design {HarmBench} to meet these criteria. Using {HarmBench}, we conduct a large-scale comparison of 18 red teaming methods and 33 target {LLMs} and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances {LLM} robustness across a wide range of attacks, demonstrating how {HarmBench} enables codevelopment of attacks and defenses. We open source {HarmBench} at https://github.com/centerforaisafety/{HarmBench}.},
	number = {{arXiv}:2402.04249},
	publisher = {{arXiv}},
	author = {Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan},
	urldate = {2025-09-03},
	date = {2024-02-27},
	eprinttype = {arxiv},
	eprint = {2402.04249 [cs]},
	keywords = {language models, evaluations, agents, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/T8G8XI6G/Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/8D2HXMNE/2402.html:text/html},
}

@misc{lindner_learning_2024,
	title = {Learning Safety Constraints from Demonstrations with Unknown Rewards},
	url = {http://arxiv.org/abs/2305.16147},
	doi = {10.48550/arXiv.2305.16147},
	abstract = {We propose Convex Constraint Learning for Reinforcement Learning ({CoCoRL}), a novel approach for inferring shared constraints in a Constrained Markov Decision Process ({CMDP}) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, {CoCoRL} can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. {CoCoRL} constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, {CoCoRL} converges to the true safe set with no policy regret. We evaluate {CoCoRL} in gridworld environments and a driving simulation with multiple constraints. {CoCoRL} learns constraints that lead to safe driving behavior. Importantly, we can safely transfer the learned constraints to different tasks and environments. In contrast, alternative methods based on Inverse Reinforcement Learning ({IRL}) often exhibit poor performance and learn unsafe policies.},
	number = {{arXiv}:2305.16147},
	publisher = {{arXiv}},
	author = {Lindner, David and Chen, Xin and Tschiatschek, Sebastian and Hofmann, Katja and Krause, Andreas},
	urldate = {2025-09-08},
	date = {2024-03-02},
	eprinttype = {arxiv},
	eprint = {2305.16147 [cs]},
	keywords = {sequential decision making, reinforcement learning, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/66F2CTKX/Lindner et al. - 2024 - Learning Safety Constraints from Demonstrations with Unknown Rewards.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/3INRK59E/2305.html:text/html},
}

@misc{novello_out--distribution_2024,
	title = {Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)},
	url = {http://arxiv.org/abs/2403.11532},
	doi = {10.48550/arXiv.2403.11532},
	shorttitle = {Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?},
	abstract = {Research on Out-Of-Distribution ({OOD}) detection focuses mainly on building scores that efficiently distinguish {OOD} data from In Distribution ({ID}) data. On the other hand, Conformal Prediction ({CP}) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use {CP} to better assess the efficiency of {OOD} scores. Specifically, we emphasize that in standard {OOD} benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal {AUROC} and conformal {FRP}@{TPR}95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference {OOD} and anomaly detection benchmarks, {OpenOOD} (Yang et al., 2022) and {ADBench} (Han et al., 2022). We also show that the benefits of using {OOD} together with {CP} apply the other way around by using {OOD} scores as non-conformity scores, which results in improving upon current {CP} methods. One of the key messages of these contributions is that since {OOD} is concerned with designing scores and {CP} with interpreting these scores, the two fields may be inherently intertwined.},
	number = {{arXiv}:2403.11532},
	publisher = {{arXiv}},
	author = {Novello, Paul and Dalmau, Joseba and Andeol, Léo},
	urldate = {2025-09-08},
	date = {2024-03-18},
	eprinttype = {arxiv},
	eprint = {2403.11532 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, probability, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/X8E6S4BH/Novello et al. - 2024 - Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa).pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/L5PUUK4P/2403.html:text/html},
}

@misc{kaur_codit_2022,
	title = {{CODiT}: Conformal Out-of-Distribution Detection in Time-Series Data},
	url = {http://arxiv.org/abs/2207.11769},
	doi = {10.48550/arXiv.2207.11769},
	shorttitle = {{CODiT}},
	abstract = {Machine learning models are prone to making incorrect predictions on inputs that are far from the training distribution. This hinders their deployment in safety-critical applications such as autonomous vehicles and healthcare. The detection of a shift from the training distribution of individual datapoints has gained attention. A number of techniques have been proposed for such out-of-distribution ({OOD}) detection. But in many applications, the inputs to a machine learning model form a temporal sequence. Existing techniques for {OOD} detection in time-series data either do not exploit temporal relationships in the sequence or do not provide any guarantees on detection. We propose using deviation from the in-distribution temporal equivariance as the non-conformity measure in conformal anomaly detection framework for {OOD} detection in time-series data.Computing independent predictions from multiple conformal detectors based on the proposed measure and combining these predictions by Fisher's method leads to the proposed detector {CODiT} with guarantees on false detection in time-series data. We illustrate the efficacy of {CODiT} by achieving state-of-the-art results on computer vision datasets in autonomous driving. We also show that {CODiT} can be used for {OOD} detection in non-vision datasets by performing experiments on the physiological {GAIT} sensory dataset. Code, data, and trained models are available at https://github.com/kaustubhsridhar/time-series-{OOD}.},
	number = {{arXiv}:2207.11769},
	publisher = {{arXiv}},
	author = {Kaur, Ramneet and Sridhar, Kaustubh and Park, Sangdon and Jha, Susmit and Roy, Anirban and Sokolsky, Oleg and Lee, Insup},
	urldate = {2025-09-08},
	date = {2022-07-24},
	eprinttype = {arxiv},
	eprint = {2207.11769 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, probability, anomaly detection},
	file = {Snapshot:/Users/ep/Zotero/storage/LW7XFNXP/2207.html:text/html},
}

@inproceedings{hennhofer_leave-one-out-_2024,
	title = {Leave-One-Out-, Bootstrap- and Cross-Conformal Anomaly Detectors},
	url = {http://arxiv.org/abs/2402.16388},
	doi = {10.1109/ICKG63256.2024.00022},
	abstract = {The requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates (\${\textbackslash}alpha\$) without compromising the statistical power (\$1-{\textbackslash}beta\$) of these systems can build trust and reduce costs related to false discoveries. The field of conformal anomaly detection emerges as a promising approach for providing respective statistical guarantees by model calibration. However, the dependency on calibration data poses practical limitations - especially within low-data regimes. In this work, we formally define and evaluate leave-one-out-, bootstrap-, and cross-conformal methods for anomaly detection, incrementing on methods from the field of conformal prediction. Looking beyond the classical inductive conformal anomaly detection, we demonstrate that derived methods for calculating resampling-conformal \$p\$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) as they make more efficient use of available data. We validate derived methods and quantify their improvements for a range of one-class classifiers and datasets.},
	pages = {110--119},
	booktitle = {2024 {IEEE} International Conference on Knowledge Graph ({ICKG})},
	author = {Hennhöfer, Oliver and Preisach, Christine},
	urldate = {2025-09-09},
	date = {2024-12-11},
	eprinttype = {arxiv},
	eprint = {2402.16388 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, probability, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/EXMEA6RH/Hennhöfer and Preisach - 2024 - Leave-One-Out-, Bootstrap- and Cross-Conformal Anomaly Detectors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/YDABAI72/2402.html:text/html},
}

@misc{adams_conformal_2025,
	title = {Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics},
	url = {http://arxiv.org/abs/2504.01172},
	doi = {10.48550/arXiv.2504.01172},
	abstract = {This paper considers the problem of outlier detection in functional data analysis focusing particularly on the more difficult case of shape outliers. We present an inductive conformal anomaly detection method based on elastic functional distance metrics. This method is evaluated and compared to similar conformal anomaly detection methods for functional data using simulation experiments. The method is also used in the analysis of two real exemplar data sets that show its utility in practical applications. The results demonstrate the efficacy of the proposed method for detecting both magnitude and shape outliers in two distinct outlier detection scenarios.},
	number = {{arXiv}:2504.01172},
	publisher = {{arXiv}},
	author = {Adams, Jason and Berman, Brandon and Michalenko, Joshua and Tucker, J. Derek},
	urldate = {2025-09-09},
	date = {2025-04-10},
	eprinttype = {arxiv},
	eprint = {2504.01172 [stat]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/ZY8T6HIN/Adams et al. - 2025 - Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/UGTS8AAU/2504.html:text/html},
}

@misc{hindy_diagnostic_2024,
	title = {Diagnostic Runtime Monitoring with Martingales},
	url = {http://arxiv.org/abs/2407.21748},
	doi = {10.48550/arXiv.2407.21748},
	abstract = {Machine learning systems deployed in safety-critical robotics settings must be robust to distribution shifts. However, system designers must understand the cause of a distribution shift in order to implement the appropriate intervention or mitigation strategy and prevent system failure. In this paper, we present a novel framework for diagnosing distribution shifts in a streaming fashion by deploying multiple stochastic martingales simultaneously. We show that knowledge of the underlying cause of a distribution shift can lead to proper interventions over the lifecycle of a deployed system. Our experimental framework can easily be adapted to different types of distribution shifts, models, and datasets. We find that our method outperforms existing work on diagnosing distribution shifts in terms of speed, accuracy, and flexibility, and validate the efficiency of our model in both simulated and live hardware settings.},
	number = {{arXiv}:2407.21748},
	publisher = {{arXiv}},
	author = {Hindy, Ali and Luo, Rachel and Banerjee, Somrita and Kuck, Jonathan and Schmerling, Edward and Pavone, Marco},
	urldate = {2025-09-09},
	date = {2024-07-31},
	eprinttype = {arxiv},
	eprint = {2407.21748 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, statistics, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/UEBDY2CB/Hindy et al. - 2024 - Diagnostic Runtime Monitoring with Martingales.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/6X8AZ8ZK/2407.html:text/html},
}

@misc{piet_jailbreaksovertime_2025,
	title = {{JailbreaksOverTime}: Detecting Jailbreak Attacks Under Distribution Shift},
	url = {http://arxiv.org/abs/2504.19440},
	doi = {10.48550/arXiv.2504.19440},
	shorttitle = {{JailbreaksOverTime}},
	abstract = {Safety and security remain critical concerns in {AI} deployment. Despite safety training through reinforcement learning with human feedback ({RLHF}) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release {JailbreaksOverTime}, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4\% to 0.3\% at a false positive rate of 0.1\%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1\%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach.},
	number = {{arXiv}:2504.19440},
	publisher = {{arXiv}},
	author = {Piet, Julien and Huang, Xiao and Jacob, Dennis and Chow, Annabella and Alrashed, Maha and Zhao, Geng and Hu, Zhanhao and Sitawarin, Chawin and Alomair, Basel and Wagner, David},
	urldate = {2025-09-10},
	date = {2025-04-28},
	eprinttype = {arxiv},
	eprint = {2504.19440 [cs]},
	keywords = {language models, robustness, adversarial, jailbreaking, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/TZFYVB7I/Piet et al. - 2025 - JailbreaksOverTime Detecting Jailbreak Attacks Under Distribution Shift.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/JKMBFL2X/2504.html:text/html},
}

@misc{luo_online_2024,
	title = {Online Distribution Shift Detection via Recency Prediction},
	url = {http://arxiv.org/abs/2211.09916},
	doi = {10.48550/arXiv.2211.09916},
	abstract = {When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate - i.e., when there is no distribution shift, our system is very unlikely (with probability \${\textless} {\textbackslash}epsilon\$) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our approach in both simulation and hardware for a visual servoing task, and show that our method indeed issues an alert before a failure occurs.},
	number = {{arXiv}:2211.09916},
	publisher = {{arXiv}},
	author = {Luo, Rachel and Sinha, Rohan and Sun, Yixiao and Hindy, Ali and Zhao, Shengjia and Savarese, Silvio and Schmerling, Edward and Pavone, Marco},
	urldate = {2025-09-12},
	date = {2024-05-18},
	eprinttype = {arxiv},
	eprint = {2211.09916 [cs]},
	keywords = {probabilistic machine learning, conformal prediction, anomaly detection},
	file = {Preprint PDF:/Users/ep/Zotero/storage/C3VW3GYC/Luo et al. - 2024 - Online Distribution Shift Detection via Recency Prediction.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/XCFLCIIR/2211.html:text/html},
}

@misc{zou_universal_2023,
	title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
	url = {http://arxiv.org/abs/2307.15043},
	doi = {10.48550/arXiv.2307.15043},
	abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against {LLMs} -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an {LLM} to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released {LLMs}. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to {ChatGPT}, Bard, and Claude, as well as open source {LLMs} such as {LLaMA}-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
	number = {{arXiv}:2307.15043},
	publisher = {{arXiv}},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	urldate = {2025-09-16},
	date = {2023-12-20},
	eprinttype = {arxiv},
	eprint = {2307.15043 [cs]},
	keywords = {language models, alignment, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/52S86J9E/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on Aligned Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/KEZPLT6C/2307.html:text/html},
}

@misc{wen_language_2024,
	title = {Language Models Learn to Mislead Humans via {RLHF}},
	url = {http://arxiv.org/abs/2409.12822},
	doi = {10.48550/arXiv.2409.12822},
	abstract = {Language models ({LMs}) can produce errors that are hard to detect for humans, especially when the task is complex. {RLHF}, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, {LMs} might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard {RLHF} pipeline, calling it "U-{SOPHISTRY}" since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task ({QuALITY}) and programming task ({APPS}), {RLHF} makes {LMs} better at convincing our subjects but not at completing the task correctly. {RLHF} also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1\% on {QuALITY} and 18.3\% on {APPS}. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored {LMs}), does not generalize to U-{SOPHISTRY}. Our results highlight an important failure mode of {RLHF} and call for more research in assisting humans to align them.},
	number = {{arXiv}:2409.12822},
	publisher = {{arXiv}},
	author = {Wen, Jiaxin and Zhong, Ruiqi and Khan, Akbir and Perez, Ethan and Steinhardt, Jacob and Huang, Minlie and Bowman, Samuel R. and He, He and Feng, Shi},
	urldate = {2025-09-16},
	date = {2024-12-08},
	eprinttype = {arxiv},
	eprint = {2409.12822 [cs]},
	keywords = {language models, reinforcement learning, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VTWX5U5G/Wen et al. - 2024 - Language Models Learn to Mislead Humans via RLHF.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/LIS65M6I/2409.html:text/html},
}

@misc{ward_honesty_2023,
	title = {Honesty Is the Best Policy: Defining and Mitigating {AI} Deception},
	url = {http://arxiv.org/abs/2312.01350},
	doi = {10.48550/arXiv.2312.01350},
	shorttitle = {Honesty Is the Best Policy},
	abstract = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of {AI} systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic {AI}, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.},
	number = {{arXiv}:2312.01350},
	publisher = {{arXiv}},
	author = {Ward, Francis Rhys and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
	urldate = {2025-09-17},
	date = {2023-12-03},
	eprinttype = {arxiv},
	eprint = {2312.01350 [cs]},
	keywords = {language models, interpretability, causality, deception, intentionality},
	file = {Preprint PDF:/Users/ep/Zotero/storage/8XZ9DPL3/Ward et al. - 2023 - Honesty Is the Best Policy Defining and Mitigating AI Deception.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/SRX5G5DM/2312.html:text/html},
}

@misc{ward_reasons_2024,
	title = {The Reasons that Agents Act: Intention and Instrumental Goals},
	url = {http://arxiv.org/abs/2402.07221},
	doi = {10.48550/arXiv.2402.07221},
	shorttitle = {The Reasons that Agents Act},
	abstract = {Intention is an important and challenging concept in {AI}. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to {AI} systems is contentious, and there is no universally accepted theory of intention applicable to {AI} agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe {AI} agents. Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour.},
	number = {{arXiv}:2402.07221},
	publisher = {{arXiv}},
	author = {Ward, Francis Rhys and {MacDermott}, Matt and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
	urldate = {2025-09-17},
	date = {2024-02-15},
	eprinttype = {arxiv},
	eprint = {2402.07221 [cs]},
	keywords = {language models, interpretability, deception, intentionality},
	file = {Preprint PDF:/Users/ep/Zotero/storage/9QAHI42F/Ward et al. - 2024 - The Reasons that Agents Act Intention and Instrumental Goals.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/EUV8N6KY/2402.html:text/html},
}

@misc{gupta_rl-obfuscation_2025,
	title = {{RL}-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?},
	url = {http://arxiv.org/abs/2506.14261},
	doi = {10.48550/arXiv.2506.14261},
	shorttitle = {{RL}-Obfuscation},
	abstract = {Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can {LLMs} learn to evade such monitors? To study this, we introduce {RL}-Obfuscation, in which {LLMs} are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply {RL}-Obfuscation to {LLMs} ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by {RL} bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.},
	number = {{arXiv}:2506.14261},
	publisher = {{arXiv}},
	author = {Gupta, Rohan and Jenner, Erik},
	urldate = {2025-09-17},
	date = {2025-06-18},
	eprinttype = {arxiv},
	eprint = {2506.14261 [cs]},
	keywords = {language models, interpretability, reinforcement learning, adversarial, jailbreaking},
	file = {Preprint PDF:/Users/ep/Zotero/storage/3J6Q448H/Gupta and Jenner - 2025 - RL-Obfuscation Can Language Models Learn to Evade Latent-Space Monitors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/62VX8247/2506.html:text/html},
}

@misc{peng_copen_2022,
	title = {{COPEN}: Probing Conceptual Knowledge in Pre-trained Language Models},
	url = {http://arxiv.org/abs/2211.04079},
	doi = {10.48550/arXiv.2211.04079},
	shorttitle = {{COPEN}},
	abstract = {Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models ({PLMs}) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of {PLMs} by designing three tasks to probe whether {PLMs} organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is {COPEN}, a {COnceptual} knowledge Probing {bENchmark}. Extensive experiments on different sizes and types of {PLMs} show that existing {PLMs} systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in {PLMs}. {COPEN} and our codes are publicly released at https://github.com/{THU}-{KEG}/{COPEN}.},
	number = {{arXiv}:2211.04079},
	publisher = {{arXiv}},
	author = {Peng, Hao and Wang, Xiaozhi and Hu, Shengding and Jin, Hailong and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Liu, Qun},
	urldate = {2025-09-22},
	date = {2022-11-08},
	eprinttype = {arxiv},
	eprint = {2211.04079 [cs]},
	keywords = {language models, interpretability, representation engineering},
	file = {Preprint PDF:/Users/ep/Zotero/storage/Y784J632/Peng et al. - 2022 - COPEN Probing Conceptual Knowledge in Pre-trained Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/7L6F5JYU/2211.html:text/html},
}

@misc{chan_can_2025,
	title = {Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models},
	url = {http://arxiv.org/abs/2507.12428},
	doi = {10.48550/arXiv.2507.12428},
	shorttitle = {Can We Predict Alignment Before Models Finish Thinking?},
	abstract = {Open-weights reasoning language models generate long chains-of-thought ({CoTs}) before producing a final response, which improves performance but introduces additional alignment risks, with harmful content often appearing in both the {CoTs} and the final outputs. In this work, we investigate if we can use {CoTs} to predict final response misalignment. We evaluate a range of monitoring approaches, including humans, highly-capable large language models, and text classifiers, using either {CoT} text or activations. First, we find that a simple linear probe trained on {CoT} activations can significantly outperform all text-based methods in predicting whether a final response will be safe or unsafe. {CoT} texts are often unfaithful and can mislead humans and classifiers, while model latents (i.e., {CoT} activations) offer a more reliable predictive signal. Second, the probe makes accurate predictions before reasoning completes, achieving strong performance even when applied to early {CoT} segments. These findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.},
	number = {{arXiv}:2507.12428},
	publisher = {{arXiv}},
	author = {Chan, Yik Siu and Yong, Zheng-Xin and Bach, Stephen H.},
	urldate = {2025-09-22},
	date = {2025-07-16},
	eprinttype = {arxiv},
	eprint = {2507.12428 [cs]},
	keywords = {language models, interpretability, reasoning, representation engineering, alignment, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MR2J94RS/Chan et al. - 2025 - Can We Predict Alignment Before Models Finish Thinking Towards Monitoring Misaligned Reasoning Mode.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/VJYGHK3E/2507.html:text/html},
}

@misc{wang_when_2025,
	title = {When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models},
	url = {http://arxiv.org/abs/2508.02087},
	doi = {10.48550/arXiv.2508.02087},
	shorttitle = {When Truth Is Overridden},
	abstract = {Large Language Models ({LLMs}) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within {LLMs}. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful {AI} systems.},
	number = {{arXiv}:2508.02087},
	publisher = {{arXiv}},
	author = {Wang, Keyu and Li, Jin and Yang, Shu and Zhang, Zhuoran and Wang, Di},
	urldate = {2025-09-22},
	date = {2025-08-05},
	eprinttype = {arxiv},
	eprint = {2508.02087 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/3LBVGTTV/Wang et al. - 2025 - When Truth Is Overridden Uncovering the Internal Origins of Sycophancy in Large Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/4HPTMKMA/2508.html:text/html},
}

@misc{wang_when_2025-1,
	title = {When Thinking {LLMs} Lie: Unveiling the Strategic Deception in Representations of Reasoning Models},
	url = {http://arxiv.org/abs/2506.04909},
	doi = {10.48550/arXiv.2506.04909},
	shorttitle = {When Thinking {LLMs} Lie},
	abstract = {The honesty of large language models ({LLMs}) is a critical alignment challenge, especially as advanced systems with chain-of-thought ({CoT}) reasoning may strategically deceive humans. Unlike traditional honesty issues on {LLMs}, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in {CoT}-enabled {LLMs}, extracting "deception vectors" via Linear Artificial Tomography ({LAT}) for 89\% detection accuracy. Through activation steering, we achieve a 40\% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy {AI} alignment.},
	number = {{arXiv}:2506.04909},
	publisher = {{arXiv}},
	author = {Wang, Kai and Zhang, Yihao and Sun, Meng},
	urldate = {2025-09-22},
	date = {2025-06-05},
	eprinttype = {arxiv},
	eprint = {2506.04909 [cs]},
	keywords = {language models, interpretability, reasoning, representation engineering, alignment, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/MHGXEFVD/Wang et al. - 2025 - When Thinking LLMs Lie Unveiling the Strategic Deception in Representations of Reasoning Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/AWH79BSX/2506.html:text/html},
}

@misc{zhong_watch_2025,
	title = {Watch the Weights: Unsupervised monitoring and control of fine-tuned {LLMs}},
	url = {http://arxiv.org/abs/2508.00161},
	doi = {10.48550/arXiv.2508.00161},
	shorttitle = {Watch the Weights},
	abstract = {The releases of powerful open-weight large language models ({LLMs}) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution. In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned {LLMs} that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision. For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100\% of attacks with a false positive rate below 1.2\%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42\% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models ({OLMo}, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation. Our implementation can be found at https://github.com/fjzzq2002/{WeightWatch}.},
	number = {{arXiv}:2508.00161},
	publisher = {{arXiv}},
	author = {Zhong, Ziqian and Raghunathan, Aditi},
	urldate = {2025-09-22},
	date = {2025-07-31},
	eprinttype = {arxiv},
	eprint = {2508.00161 [cs]},
	keywords = {language models, interpretability, representation engineering, evaluations, alignment, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/P42J2D36/Zhong and Raghunathan - 2025 - Watch the Weights Unsupervised monitoring and control of fine-tuned LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/S6YTFSZK/2508.html:text/html},
}

@misc{beaglehole_toward_2025,
	title = {Toward universal steering and monitoring of {AI} models},
	url = {http://arxiv.org/abs/2502.03708},
	doi = {10.48550/arXiv.2502.03708},
	abstract = {Modern {AI} models contain much of human knowledge, yet understanding of their internal representation of this knowledge remains elusive. Characterizing the structure and properties of this representation will lead to improvements in model capabilities and development of effective safeguards. Building on recent advances in feature learning, we develop an effective, scalable approach for extracting linear representations of general concepts in large-scale {AI} models (language models, vision-language models, and reasoning models). We show how these representations enable model steering, through which we expose vulnerabilities, mitigate misaligned behaviors, and improve model capabilities. Additionally, we demonstrate that concept representations are remarkably transferable across human languages and combinable to enable multi-concept steering. Through quantitative analysis across hundreds of concepts, we find that newer, larger models are more steerable and steering can improve model capabilities beyond standard prompting. We show how concept representations are effective for monitoring misaligned content (hallucinations, toxic content). We demonstrate that predictive models built using concept representations are more accurate for monitoring misaligned content than using models that judge outputs directly. Together, our results illustrate the power of using internal representations to map the knowledge in {AI} models, advance {AI} safety, and improve model capabilities.},
	number = {{arXiv}:2502.03708},
	publisher = {{arXiv}},
	author = {Beaglehole, Daniel and Radhakrishnan, Adityanarayanan and Boix-Adserà, Enric and Belkin, Mikhail},
	urldate = {2025-09-22},
	date = {2025-05-28},
	eprinttype = {arxiv},
	eprint = {2502.03708 [cs]},
	keywords = {language models, interpretability, representation engineering, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/XTFW9DPA/Beaglehole et al. - 2025 - Toward universal steering and monitoring of AI models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9S89PI8I/2502.html:text/html},
}

@misc{chaudhary_safetynet_2025,
	title = {{SafetyNet}: Detecting Harmful Outputs in {LLMs} by Modeling and Monitoring Deceptive Behaviors},
	url = {http://arxiv.org/abs/2505.14300},
	doi = {10.48550/arXiv.2505.14300},
	shorttitle = {{SafetyNet}},
	abstract = {High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models ({LLMs}) need monitoring safeguards. We propose a real-time framework to predict harmful {AI} outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether {LLMs} display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96\% accuracy in detecting harmful cases using our unsupervised ensemble approach.},
	number = {{arXiv}:2505.14300},
	publisher = {{arXiv}},
	author = {Chaudhary, Maheep and Barez, Fazl},
	urldate = {2025-09-22},
	date = {2025-05-20},
	eprinttype = {arxiv},
	eprint = {2505.14300 [cs]},
	keywords = {language models, interpretability, representation engineering, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/FHF3D5HN/Chaudhary and Barez - 2025 - SafetyNet Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/AX6QX9WS/2505.html:text/html},
}

@misc{clymer_poser_2024,
	title = {Poser: Unmasking Alignment Faking {LLMs} by Manipulating Their Internals},
	url = {http://arxiv.org/abs/2405.05466},
	doi = {10.48550/arXiv.2405.05466},
	shorttitle = {Poser},
	abstract = {Like a criminal under investigation, Large Language Models ({LLMs}) might pretend to be aligned while evaluated and misbehave when they have a good opportunity. Can current interpretability methods catch these 'alignment fakers?' To answer this question, we introduce a benchmark that consists of 324 pairs of {LLMs} fine-tuned to select actions in role-play scenarios. One model in each pair is consistently benign (aligned). The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking). The task is to identify the alignment faking model using only inputs where the two models behave identically. We test five detection strategies, one of which identifies 98\% of alignment-fakers.},
	number = {{arXiv}:2405.05466},
	publisher = {{arXiv}},
	author = {Clymer, Joshua and Juang, Caden and Field, Severin},
	urldate = {2025-09-22},
	date = {2024-05-11},
	eprinttype = {arxiv},
	eprint = {2405.05466 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/Y8CLDHWV/Clymer et al. - 2024 - Poser Unmasking Alignment Faking LLMs by Manipulating Their Internals.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/2JMG7PHC/2405.html:text/html},
}

@misc{xie_sorry-bench_2025,
	title = {{SORRY}-Bench: Systematically Evaluating Large Language Model Safety Refusal},
	url = {http://arxiv.org/abs/2406.14598},
	doi = {10.48550/arXiv.2406.14598},
	shorttitle = {{SORRY}-Bench},
	abstract = {Evaluating aligned large language models' ({LLMs}) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with {SORRY}-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. {SORRY}-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement {SORRY}-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large {LLMs} (e.g., {GPT}-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse {LLM}-as-a-judge designs, we show that fine-tuned 7B {LLMs} can achieve accuracy comparable to {GPT}-4 scale {LLMs}, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight {LLMs} on {SORRY}-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of {LLMs}' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through https://sorry-bench.github.io.},
	number = {{arXiv}:2406.14598},
	publisher = {{arXiv}},
	author = {Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and Jia, Ruoxi and Li, Bo and Li, Kai and Chen, Danqi and Henderson, Peter and Mittal, Prateek},
	urldate = {2025-09-22},
	date = {2025-03-01},
	eprinttype = {arxiv},
	eprint = {2406.14598 [cs]},
	keywords = {language models, reasoning, evaluations, dangerous capabilities},
	file = {Preprint PDF:/Users/ep/Zotero/storage/RRVHBCAG/Xie et al. - 2025 - SORRY-Bench Systematically Evaluating Large Language Model Safety Refusal.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/68YVCKI7/2406.html:text/html},
}

@misc{wu_opendeception_2025,
	title = {{OpenDeception}: Benchmarking and Investigating {AI} Deceptive Behaviors via Open-ended Interaction Simulation},
	url = {http://arxiv.org/abs/2504.13707},
	doi = {10.48550/arXiv.2504.13707},
	shorttitle = {{OpenDeception}},
	abstract = {As the general capabilities of large language models ({LLMs}) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce {OpenDeception}, a novel deception evaluation framework with an open-ended scenario dataset. {OpenDeception} jointly evaluates both the deception intention and capabilities of {LLM}-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where {LLMs} intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream {LLMs} on {OpenDeception} highlights the urgent need to address deception risks and security concerns in {LLM}-based agents: the deception intention ratio across the models exceeds 80\%, while the deception success rate surpasses 50\%. Furthermore, we observe that {LLMs} with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.},
	number = {{arXiv}:2504.13707},
	publisher = {{arXiv}},
	author = {Wu, Yichen and Pan, Xudong and Hong, Geng and Yang, Min},
	urldate = {2025-09-22},
	date = {2025-09-08},
	eprinttype = {arxiv},
	eprint = {2504.13707 [cs]},
	keywords = {language models, evaluations, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/7X85I4IM/Wu et al. - 2025 - OpenDeception Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simu.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/JINYR6G6/2504.html:text/html},
}

@misc{ji_mitigating_2025,
	title = {Mitigating Deceptive Alignment via Self-Monitoring},
	url = {http://arxiv.org/abs/2505.18807},
	doi = {10.48550/arXiv.2505.18807},
	abstract = {Modern large language models rely on chain-of-thought ({CoT}) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the {CoT} process itself, named {CoT} Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce {DeceptionBench}, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various {LLMs} and show that unrestricted {CoT} roughly aggravates the deceptive tendency. In contrast, {CoT} Monitor+ cuts deceptive behaviors by 43.8\% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in {RL} fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io},
	number = {{arXiv}:2505.18807},
	publisher = {{arXiv}},
	author = {Ji, Jiaming and Chen, Wenqi and Wang, Kaile and Hong, Donghai and Fang, Sitong and Chen, Boyuan and Zhou, Jiayi and Dai, Juntao and Han, Sirui and Guo, Yike and Yang, Yaodong},
	urldate = {2025-09-22},
	date = {2025-05-24},
	eprinttype = {arxiv},
	eprint = {2505.18807 [cs]},
	keywords = {language models, interpretability, representation engineering, alignment, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/VDRBA49V/Ji et al. - 2025 - Mitigating Deceptive Alignment via Self-Monitoring.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/9JQKHIW8/2505.html:text/html},
}

@misc{kran_darkbench_2025,
	title = {{DarkBench}: Benchmarking Dark Patterns in Large Language Models},
	url = {http://arxiv.org/abs/2503.10728},
	doi = {10.48550/arXiv.2503.10728},
	shorttitle = {{DarkBench}},
	abstract = {We introduce {DarkBench}, a comprehensive benchmark for detecting dark design patterns--manipulative techniques that influence user behavior--in interactions with large language models ({LLMs}). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies ({OpenAI}, Anthropic, Meta, Mistral, Google) and find that some {LLMs} are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing {LLMs} should recognize and mitigate the impact of dark design patterns to promote more ethical {AI}.},
	number = {{arXiv}:2503.10728},
	publisher = {{arXiv}},
	author = {Kran, Esben and Nguyen, Hieu Minh "Jord" and Kundu, Akash and Jawhar, Sami and Park, Jinsuk and Jurewicz, Mateusz Maria},
	urldate = {2025-09-22},
	date = {2025-03-13},
	eprinttype = {arxiv},
	eprint = {2503.10728 [cs]},
	keywords = {evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/5GTZF4B4/Kran et al. - 2025 - DarkBench Benchmarking Dark Patterns in Large Language Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/LNR3F54H/2503.html:text/html},
}

@misc{he_evaluating_2025,
	title = {Evaluating the Paperclip Maximizer: Are {RL}-Based Language Models More Likely to Pursue Instrumental Goals?},
	url = {http://arxiv.org/abs/2502.12206},
	doi = {10.48550/arXiv.2502.12206},
	shorttitle = {Evaluating the Paperclip Maximizer},
	abstract = {As large language models ({LLMs}) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is {\textbackslash}textit\{instrumental convergence\}, where an {AI} system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning ({RL})-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in {LLMs} by comparing models trained with direct {RL} optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback ({RLHF}). We hypothesize that {RL}-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce {InstrumentalEval}, a benchmark for evaluating instrumental convergence in {RL}-trained {LLMs}. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence. Our findings contribute to a deeper understanding of alignment challenges in {AI} systems and the risks posed by unintended model behaviors.},
	number = {{arXiv}:2502.12206},
	publisher = {{arXiv}},
	author = {He, Yufei and Li, Yuexin and Wu, Jiaying and Sui, Yuan and Chen, Yulin and Hooi, Bryan},
	urldate = {2025-09-22},
	date = {2025-02-16},
	eprinttype = {arxiv},
	eprint = {2502.12206 [cs]},
	keywords = {language models, reasoning, reinforcement learning, evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/SNWYKQE4/He et al. - 2025 - Evaluating the Paperclip Maximizer Are RL-Based Language Models More Likely to Pursue Instrumental.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/R6JRABB4/2502.html:text/html},
}

@misc{abdelnabi_linear_2025,
	title = {Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models},
	url = {http://arxiv.org/abs/2505.14617},
	doi = {10.48550/arXiv.2505.14617},
	abstract = {Reasoning-focused large language models ({LLMs}) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning {LLMs} across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.},
	number = {{arXiv}:2505.14617},
	publisher = {{arXiv}},
	author = {Abdelnabi, Sahar and Salem, Ahmed},
	urldate = {2025-09-22},
	date = {2025-05-26},
	eprinttype = {arxiv},
	eprint = {2505.14617 [cs]},
	keywords = {representation engineering, dangerous capabilities, self awareness},
	file = {Preprint PDF:/Users/ep/Zotero/storage/GWAVR2UA/Abdelnabi and Salem - 2025 - Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/K2BY6EP6/2505.html:text/html},
}

@misc{lian_semantic_2025,
	title = {Semantic Representation Attack against Aligned Large Language Models},
	url = {http://arxiv.org/abs/2509.19360},
	doi = {10.48550/arXiv.2509.19360},
	abstract = {Large Language Models ({LLMs}) increasingly employ alignment techniques to prevent harmful outputs. Despite these safeguards, attackers can circumvent them by crafting prompts that induce {LLMs} to generate harmful content. Current methods typically target exact affirmative responses, such as ``Sure, here is...'', suffering from limited convergence, unnatural prompts, and high computational costs. We introduce Semantic Representation Attack, a novel paradigm that fundamentally reconceptualizes adversarial objectives against aligned {LLMs}. Rather than targeting exact textual patterns, our approach exploits the semantic representation space comprising diverse responses with equivalent harmful meanings. This innovation resolves the inherent trade-off between attack efficacy and prompt naturalness that plagues existing methods. The Semantic Representation Heuristic Search algorithm is proposed to efficiently generate semantically coherent and concise adversarial prompts by maintaining interpretability during incremental expansion. We establish rigorous theoretical guarantees for semantic convergence and demonstrate that our method achieves unprecedented attack success rates (89.41{\textbackslash}\% averaged across 18 {LLMs}, including 100{\textbackslash}\% on 11 models) while maintaining stealthiness and efficiency. Comprehensive experimental results confirm the overall superiority of our Semantic Representation Attack. The code will be publicly available.},
	number = {{arXiv}:2509.19360},
	author = {Lian, Jiawei and Pan, Jianhong and Wang, Lefan and Wang, Yi and Mei, Shaohui and Chau, Lap-Pui},
	urldate = {2025-09-28},
	date = {2025-09-18},
	eprinttype = {arxiv},
	eprint = {2509.19360 [cs]},
	keywords = {language models, adversarial, jailbreaking},
	file = {Lian et al. - 2025 - Semantic Representation Attack against Aligned Large Language Models.pdf:/Users/ep/Zotero/storage/D88QRDGB/Lian et al. - 2025 - Semantic Representation Attack against Aligned Large Language Models.pdf:application/pdf},
}

@misc{li_llms_2025,
	title = {{LLMs} Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring},
	url = {http://arxiv.org/abs/2508.00943},
	doi = {10.48550/arXiv.2508.00943},
	abstract = {Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an {AI} system is safe to deploy. One empirically demonstrated threat to this is sandbagging - the strategic underperformance on evaluations by {AI} models or their developers. One promising defense is to monitor a model's chain-of-thought ({CoT}) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a {CoT} monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against {CoT} monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36{\textbackslash}\% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught {CoTs} to understand why the monitor failed. We reveal a rich attack surface for {CoT} monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of {CoT} monitoring and may help build more diverse sandbagging model organisms.},
	number = {{arXiv}:2508.00943},
	publisher = {{arXiv}},
	author = {Li, Chloe and Phuong, Mary and Siegel, Noah Y.},
	urldate = {2025-09-28},
	date = {2025-07-31},
	eprinttype = {arxiv},
	eprint = {2508.00943 [cs]},
	keywords = {language models, evaluations, dangerous capabilities, deception},
	file = {Preprint PDF:/Users/ep/Zotero/storage/UAIGL64C/Li et al. - 2025 - LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/QWBFLYSL/2508.html:text/html},
}

@misc{hong_measuring_2025,
	title = {Measuring Sycophancy of Language Models in Multi-turn Dialogues},
	url = {http://arxiv.org/abs/2505.23840},
	doi = {10.48550/arXiv.2505.23840},
	abstract = {Large Language Models ({LLMs}) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce {SYCON} Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying {SYCON} Bench to 17 {LLMs} across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8\% in debate scenario. We release our code and data at https://github.com/{JiseungHong}/{SYCON}-Bench.},
	number = {{arXiv}:2505.23840},
	publisher = {{arXiv}},
	author = {Hong, Jiseung and Byun, Grace and Kim, Seungone and Shu, Kai and Choi, Jinho D.},
	urldate = {2025-09-28},
	date = {2025-08-26},
	eprinttype = {arxiv},
	eprint = {2505.23840 [cs]},
	keywords = {language models, evaluations, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/2NTNFS3U/Hong et al. - 2025 - Measuring Sycophancy of Language Models in Multi-turn Dialogues.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/8526PCWH/2505.html:text/html},
}

@misc{papadatos_linear_2024,
	title = {Linear Probe Penalties Reduce {LLM} Sycophancy},
	url = {http://arxiv.org/abs/2412.00967},
	doi = {10.48550/arXiv.2412.00967},
	abstract = {Large language models ({LLMs}) are often sycophantic, prioritizing agreement with their users over accurate or objective statements. This problematic behavior becomes more pronounced during reinforcement learning from human feedback ({RLHF}), an {LLM} fine-tuning stage intended to align model outputs with human values. Instead of increasing accuracy and reliability, the reward model learned from {RLHF} often rewards sycophancy. We develop a linear probing method to identify and penalize markers of sycophancy within the reward model, producing rewards that discourage sycophantic behavior. Our experiments show that constructing and optimizing against this surrogate reward function reduces sycophantic behavior in multiple open-source {LLMs}. Our results suggest a generalizable methodology for reducing unwanted {LLM} behaviors that are not sufficiently disincentivized by {RLHF} fine-tuning.},
	number = {{arXiv}:2412.00967},
	publisher = {{arXiv}},
	author = {Papadatos, Henry and Freedman, Rachel},
	urldate = {2025-09-28},
	date = {2024-12-01},
	eprinttype = {arxiv},
	eprint = {2412.00967 [cs]},
	keywords = {language models, interpretability, dangerous capabilities, safety guarantees, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/S4DIA7CG/Papadatos and Freedman - 2024 - Linear Probe Penalties Reduce LLM Sycophancy.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/CJEC3RKX/2412.html:text/html},
}

@misc{vennemeyer_sycophancy_2025,
	title = {Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in {LLMs}},
	url = {http://arxiv.org/abs/2509.21305},
	doi = {10.48550/arXiv.2509.21305},
	shorttitle = {Sycophancy Is Not One Thing},
	abstract = {Large language models ({LLMs}) often exhibit sycophantic behaviors -- such as excessive agreement with or flattery of the user -- but it is unclear whether these behaviors arise from a single mechanism or multiple distinct processes. We decompose sycophancy into sycophantic agreement and sycophantic praise, contrasting both with genuine agreement. Using difference-in-means directions, activation additions, and subspace geometry across multiple models and datasets, we show that: (1) the three behaviors are encoded along distinct linear directions in latent space; (2) each behavior can be independently amplified or suppressed without affecting the others; and (3) their representational structure is consistent across model families and scales. These results suggest that sycophantic behaviors correspond to distinct, independently steerable representations.},
	number = {{arXiv}:2509.21305},
	publisher = {{arXiv}},
	author = {Vennemeyer, Daniel and Duong, Phan Anh and Zhan, Tiffany and Jiang, Tianyu},
	urldate = {2025-09-29},
	date = {2025-09-26},
	eprinttype = {arxiv},
	eprint = {2509.21305 [cs]},
	keywords = {language models, interpretability, causality, causal representations, representation engineering, evaluations, dangerous capabilities, sycophancy},
	file = {Preprint PDF:/Users/ep/Zotero/storage/LXPQSDR3/Vennemeyer et al. - 2025 - Sycophancy Is Not One Thing Causal Separation of Sycophantic Behaviors in LLMs.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/FLMCCY6W/2509.html:text/html},
}

@misc{das_latent_2025,
	title = {Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation},
	url = {http://arxiv.org/abs/2509.20623},
	doi = {10.48550/arXiv.2509.20623},
	shorttitle = {Latent Activation Editing},
	abstract = {Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing ({LAE}) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that {LAE} achieves statistically significant reduction in collisions (nearly 90\% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish {LAE} as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.},
	number = {{arXiv}:2509.20623},
	publisher = {{arXiv}},
	author = {Das, Satyajeet and Chiu, Darren and Huang, Zhehui and Lindemann, Lars and Sukhatme, Gaurav S.},
	urldate = {2025-09-29},
	date = {2025-09-24},
	eprinttype = {arxiv},
	eprint = {2509.20623 [cs]},
	keywords = {interpretability, representation engineering, robotics, multi-agent, safety guarantees},
	file = {Preprint PDF:/Users/ep/Zotero/storage/V8KPPRDU/Das et al. - 2025 - Latent Activation Editing Inference-Time Refinement of Learned Policies for Safer Multirobot Naviga.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/X2J5XJNB/2509.html:text/html},
}

@misc{oldfield_beyond_2025,
	title = {Beyond Linear Probes: Dynamic Safety Monitoring for Language Models},
	url = {http://arxiv.org/abs/2509.26238},
	doi = {10.48550/arXiv.2509.26238},
	shorttitle = {Beyond Linear Probes},
	abstract = {Monitoring large language models' ({LLMs}) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers ({TPCs}), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. {TPCs} provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets ({WildGuardMix} and {BeaverTails}), for 4 models with up to 30B parameters, we show that {TPCs} compete with or outperform {MLP}-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.},
	number = {{arXiv}:2509.26238},
	author = {Oldfield, James and Torr, Philip and Patras, Ioannis and Bibi, Adel and Barez, Fazl},
	urldate = {2025-10-01},
	date = {2025-09-30},
	eprinttype = {arxiv},
	eprint = {2509.26238 [cs]},
	keywords = {language models, interpretability, reasoning, safety guarantees},
	file = {Oldfield et al. - 2025 - Beyond Linear Probes Dynamic Safety Monitoring for Language Models.pdf:/Users/ep/Zotero/storage/MD8UW997/Oldfield et al. - 2025 - Beyond Linear Probes Dynamic Safety Monitoring for Language Models.pdf:application/pdf},
}

@misc{yan_when_2025,
	title = {When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced Misalignment},
	url = {http://arxiv.org/abs/2509.00544},
	doi = {10.48550/arXiv.2509.00544},
	shorttitle = {When Thinking Backfires},
	abstract = {With the growing accessibility and wide adoption of large language models, concerns about their safety and alignment with human values have become paramount. In this paper, we identify a concerning phenomenon: Reasoning-Induced Misalignment ({RIM}), in which misalignment emerges when reasoning capabilities strengthened-particularly when specific types of reasoning patterns are introduced during inference or training. Beyond reporting this vulnerability, we provide the first mechanistic account of its origins. Through representation analysis, we discover that specific attention heads facilitate refusal by reducing their attention to {CoT} tokens, a mechanism that modulates the model's rationalization process during inference. During training, we find significantly higher activation entanglement between reasoning and safety in safety-critical neurons than in control neurons, particularly after fine-tuning with those identified reasoning patterns. This entanglement strongly correlates with catastrophic forgetting, providing a neuron-level explanation for {RIM}.},
	number = {{arXiv}:2509.00544},
	publisher = {{arXiv}},
	author = {Yan, Hanqi and Xu, Hainiu and Qi, Siya and Yang, Shu and He, Yulan},
	urldate = {2025-10-02},
	date = {2025-09-28},
	eprinttype = {arxiv},
	eprint = {2509.00544 [cs]},
	keywords = {language models, interpretability, reasoning, alignment},
	file = {Preprint PDF:/Users/ep/Zotero/storage/6SITJD9N/Yan et al. - 2025 - When Thinking Backfires Mechanistic Insights Into Reasoning-Induced Misalignment.pdf:application/pdf;Snapshot:/Users/ep/Zotero/storage/MXCECKF7/2509.html:text/html},
}
